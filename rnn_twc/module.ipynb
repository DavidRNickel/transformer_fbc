{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import special as sp # for qfunc\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 17px;\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%html\n",
    "# <style type='text/css'>\n",
    "# .CodeMirror{\n",
    "# font-size: 17px;\n",
    "# </style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class params():\n",
    "#     def __init__(self):\n",
    "\n",
    "#         # Encoder\n",
    "#         self.encoder_act_func = 'tanh'\n",
    "#         self.encoder_N_layers: int = 2    # number of RNN layers at encoder\n",
    "#         self.encoder_N_neurons: int = 50  # number of neurons at each RNN\n",
    "        \n",
    "#         # Decoder\n",
    "#         self.decoder_N_layers: int = 2    # number of RNN layers at decoder\n",
    "#         self.decoder_N_neurons: int = 50  # number of neurons at each RNN\n",
    "#         self.decoder_bidirection = True   # True: bi-directional decoding, False: uni-directional decoding\n",
    "#         self.attention_type: int = 5      # choose the attention type among five options\n",
    "#         # 1. Only the last timestep (N-th)\n",
    "#         # 2. Merge the last outputs of forward/backward RNN\n",
    "#         # 3. Sum over all timesteps\n",
    "#         # 4. Attention mechanism with N weights (same weight for forward/backward)\n",
    "#         # 5. Attention mechanism with 2N weights (separate weights for forward/backward)\n",
    "        \n",
    "#         # Setup\n",
    "#         self.N_bits: int = 6                # number of bits\n",
    "#         self.N_channel_use = 18             # number of channel uses\n",
    "#         self.input_type = 'bit_vector'      # choose 'bit_vector' or 'one_hot_vector'\n",
    "#         self.output_type = 'one_hot_vector' # choose 'bit_vector' or 'one_hot_vector'\n",
    "\n",
    "#         # Learning parameters\n",
    "#         self.batch_size = int(2.5e4) \n",
    "#         self.learning_rate = 0.01 \n",
    "#         self.use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qfunc(x):\n",
    "    return 0.5-0.5*sp.erf(x/np.sqrt(2))\n",
    "\n",
    "def SER(B, SNR):\n",
    "    a = np.sqrt(3*(2**B-1)/(2**B+1)) # max amplitude so that E[m^2] =1\n",
    "    d = 2*a/(2**B-1) # symbol distance\n",
    "    ser = (2**(B+1)-2)/2**B*qfunc(d/2*np.sqrt(SNR))\n",
    "    return ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error_rate_bitvector(b_est, b):\n",
    "    b = np.round(b)          # (batch,K)\n",
    "    b_est = np.round(b_est)  # (batch,K)\n",
    "\n",
    "    error_matrix = np.not_equal(b, b_est).float() # (batch,K)\n",
    "    N_batch = error_matrix.shape[0]\n",
    "    N_bits = error_matrix.shape[1]\n",
    "    ber = torch.sum(torch.sum(error_matrix))/(N_batch*N_bits) \n",
    "    bler = torch.sum((torch.sum(error_matrix, axis=1)>0))/N_batch\n",
    "    return ber, bler\n",
    "\n",
    "def error_rate_onehot(d_est, b): # b -- (batch, K, 1)\n",
    "\n",
    "    ind_est = torch.argmax(d_est, dim=1).squeeze(-1) # batch\n",
    "    \n",
    "    N_batch = b.size(0) \n",
    "    N_bits = b.size(1)\n",
    "    b_est = dec2bin(ind_est, N_bits)                 # (batch, K)\n",
    "    b = b.squeeze(-1)                                # (batch, K)\n",
    "    \n",
    "    error_matrix = np.not_equal(b, b_est).float() # batch,K\n",
    "    ber = torch.sum(torch.sum(error_matrix))/(N_batch*N_bits) \n",
    "    bler = torch.sum((torch.sum(error_matrix, dim=1)>0))/N_batch\n",
    "\n",
    "    return ber, bler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decimal to binary given bits, e.g., x=8 --> 00010 in 5 bits (Note. reverse representation)\n",
    "def dec2bin(x, N_bits):\n",
    "    mask = 2**torch.arange(N_bits) # .to(device)\n",
    "    return x.unsqueeze(-1).bitwise_and(mask).ne(0).byte() # add another axis, multiply bit seq, and denote it as binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-torch",
   "language": "python",
   "name": "my-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
