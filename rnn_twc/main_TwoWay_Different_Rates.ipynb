{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from ipynb.fs.full.module import * # !pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 17px;\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%html\n",
    "# <style type='text/css'>\n",
    "# .CodeMirror{\n",
    "# font-size: 17px;\n",
    "# </style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class params():\n",
    "    def __init__(self):\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_act_func = 'tanh'\n",
    "        self.encoder_N_layers: int = 2    # number of RNN layers at encoder\n",
    "        self.encoder_N_neurons: int = 50  # number of neurons at each RNN\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_N_layers: int = 2    # number of RNN layers at decoder\n",
    "        self.decoder_N_neurons: int = 50  # number of neurons at each RNN\n",
    "        self.decoder_bidirection = True   # True: bi-directional decoding, False: uni-directional decoding\n",
    "        self.attention_type: int = 5      # choose the attention type among five options\n",
    "        # 1. Only the last timestep (N-th)\n",
    "        # 2. Merge the last outputs of forward/backward RNN\n",
    "        # 3. Sum over all timesteps\n",
    "        # 4. Attention mechanism with N weights (same weight for forward/backward)\n",
    "        # 5. Attention mechanism with 2N weights (separate weights for forward/backward)\n",
    "        # Uni --> Choose False, 4\n",
    "        \n",
    "        # Setup\n",
    "        self.K1: int = 6                # number of bits\n",
    "        self.K2: int = 6                # number of bits\n",
    "        self.N_channel_use = 18             # number of channel uses\n",
    "        self.input_type = 'bit_vector'      # choose 'bit_vector' or 'one_hot_vector'\n",
    "        self.output_type = 'one_hot_vector' # choose 'bit_vector' or 'one_hot_vector'\n",
    "        self.decoder_info = 'None'          # 'bit_estimate', 'state_vector', 'None' for encoder input\n",
    "        self.encoder_info = 'tran_symbol'   # 'tran_symbol', 'state_vector', 'None' for decoder input\n",
    "\n",
    "        # Learning parameters\n",
    "        self.batch_size = int(2.5e4) \n",
    "#         self.batch_size = int(2e4) \n",
    "#         self.batch_size = int(5e4)  # -- train with 1e8, sig, bi // test with 2e5\n",
    "#         self.batch_size = int(1e5)\n",
    "#         self.batch_size = int(2e5)\n",
    "        self.learning_rate = 0.01 \n",
    "        self.use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on decoder_info, the architecture is restricted\n",
    "# 1. If self.decoder_info = 'None', no restriction\n",
    "# 2. If self.decoder_info = 'bit_estimate', consider only uni-directional and immediate decoding\n",
    "# 3. If self.decoder_info = 'state_vector', consider only uni-directional (No need for immediate decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "parameter = params()\n",
    "use_cuda = parameter.use_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:1\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np1:  3.1622776601683795\n",
      "np2:  0.001\n"
     ]
    }
   ],
   "source": [
    "# Generate training data\n",
    "SNR1 = -5               # SNR at User1 in dB\n",
    "np1 = 10**(-SNR1/10)   # noise power1 -- Assuming signal power is set to 1\n",
    "sigma1 = np.sqrt(np1)\n",
    "SNR2 = 30               # SNR at User2 in dB\n",
    "np2 = 10**(-SNR2/10)\n",
    "sigma2 = np.sqrt(np2)\n",
    "\n",
    "# Training set: tuples of (stream, noise1, noise 2)\n",
    "N_train = int(1e7)  # number of training set\n",
    "# N_train = int(1e9)  # number of training set\n",
    "# N_train = int(1e5)\n",
    "bit1_train     = torch.randint(0, 2, (N_train, parameter.K1, 1))\n",
    "bit2_train     = torch.randint(0, 2, (N_train, parameter.K2, 1))\n",
    "noise1_train   = sigma1*torch.randn((N_train, parameter.N_channel_use, 1)) \n",
    "noise2_train   = sigma2*torch.randn((N_train, parameter.N_channel_use, 1)) \n",
    "\n",
    "# Validation\n",
    "N_validation = int(1e5)\n",
    "\n",
    "print('np1: ', np1)\n",
    "print('np2: ', np2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twoway_coding(torch.nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(Twoway_coding, self).__init__()\n",
    "        \n",
    "        # import parameter\n",
    "        self.param        = param\n",
    "        if self.param.decoder_bidirection == True:\n",
    "            self.decoder_bi = 2 # bi-direction\n",
    "        else:\n",
    "            self.decoder_bi = 1 # uni-direction\n",
    "\n",
    "        ### Encoder input type\n",
    "        # 1. input_type (bit vector, one-hot vector) for Encoder\n",
    "        if self.param.input_type == 'bit_vector':\n",
    "            self.num_input1 = self.param.K1\n",
    "            self.num_input2 = self.param.K2\n",
    "        elif self.param.input_type == 'one_hot_vector':\n",
    "            self.num_input1 = 2**self.param.K1\n",
    "            self.num_input2 = 2**self.param.K2\n",
    "        \n",
    "        # 2. Decoder Info (bit estimate, state vector) for Encoder\n",
    "        if self.param.decoder_info == 'bit_estimate':\n",
    "            self.num_D = self.param.N_bits\n",
    "        elif self.param.decoder_info == 'state_vector':\n",
    "            self.num_D = self.decoder_bi * self.param.decoder_N_neurons # 2*50 = 100\n",
    "        elif self.param.decoder_info == 'None':\n",
    "            self.num_D = 0\n",
    "        \n",
    "        ### Decoder input type\n",
    "        # 1. output_type (bits, one-hot vector) for Decoder\n",
    "        if self.param.output_type == 'bit_vector':\n",
    "            self.num_output1 = self.param.K2\n",
    "            self.num_output2 = self.param.K1\n",
    "        elif self.param.output_type == 'one_hot_vector':\n",
    "            self.num_output1 = 2**self.param.K2\n",
    "            self.num_output2 = 2**self.param.K1\n",
    "            \n",
    "        # 2. Encoder Info ('tran_symbol', 'state_vector', 'None') for Decoder\n",
    "        if self.param.encoder_info == 'tran_symbol':\n",
    "            self.num_E = 1\n",
    "        elif self.param.encoder_info == 'state_vector':\n",
    "            self.num_E = self.param.encoder_N_neurons # 50\n",
    "        elif self.param.encoder_info == 'None':\n",
    "            self.num_E = 0\n",
    "            \n",
    "\n",
    "        # encoder 1. RNN\n",
    "        self.encoder1_RNN   = torch.nn.GRU(self.num_input1 + 1 + self.num_D, self.param.encoder_N_neurons, num_layers = self.param.encoder_N_layers, \n",
    "                                          bias=True, batch_first=True, dropout=0, bidirectional=False)\n",
    "        self.encoder1_linear = torch.nn.Linear(self.param.encoder_N_neurons, 1)\n",
    "        \n",
    "        # encoder 2. RNN\n",
    "        self.encoder2_RNN   = torch.nn.GRU(self.num_input2 + 1 + self.num_D, self.param.encoder_N_neurons, num_layers = self.param.encoder_N_layers, \n",
    "                                          bias=True, batch_first=True, dropout=0, bidirectional=False)\n",
    "        self.encoder2_linear = torch.nn.Linear(self.param.encoder_N_neurons, 1)\n",
    "\n",
    "        # power weight 1\n",
    "        self.weight_power1 = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use), requires_grad = True )\n",
    "        self.weight_power1.data.uniform_(1.0, 1.0) # all 1\n",
    "        self.weight_power1_normalized = torch.sqrt(self.weight_power1**2 *(self.param.N_channel_use)/torch.sum(self.weight_power1**2))\n",
    "        \n",
    "        # power weight 2\n",
    "        self.weight_power2 = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use), requires_grad = True )\n",
    "        self.weight_power2.data.uniform_(1.0, 1.0) # all 1\n",
    "        self.weight_power2_normalized = torch.sqrt(self.weight_power2**2 *(self.param.N_channel_use)/torch.sum(self.weight_power2**2))\n",
    "        \n",
    "        # decoder 1\n",
    "        self.decoder1_RNN = torch.nn.GRU(self.num_input1 + 1 + self.num_E, self.param.decoder_N_neurons, num_layers = self.param.decoder_N_layers, \n",
    "                                        bias=True, batch_first=True, dropout=0, bidirectional= self.param.decoder_bidirection) \n",
    "        self.decoder1_linear = torch.nn.Linear(self.decoder_bi*self.param.decoder_N_neurons, self.num_output1) # 100,10\n",
    "        \n",
    "        # decoder 2\n",
    "        self.decoder2_RNN = torch.nn.GRU(self.num_input2 + 1 + self.num_E, self.param.decoder_N_neurons, num_layers = self.param.decoder_N_layers, \n",
    "                                        bias=True, batch_first=True, dropout=0, bidirectional= self.param.decoder_bidirection) \n",
    "        self.decoder2_linear = torch.nn.Linear(self.decoder_bi*self.param.decoder_N_neurons, self.num_output2) # 100,10\n",
    "\n",
    "        \n",
    "        # attention type\n",
    "        if self.param.attention_type==5:  # bi-directional --> 2N weights\n",
    "            self.weight1_merge = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use,2), requires_grad = True ) \n",
    "            self.weight1_merge.data.uniform_(1.0, 1.0) # all 1\n",
    "            # Normalization\n",
    "            self.weight1_merge_normalized_fwd = torch.sqrt(self.weight1_merge[:,0]**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge[:,0]**2)) \n",
    "            self.weight1_merge_normalized_bwd  = torch.sqrt(self.weight1_merge[:,1]**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge[:,1]**2))\n",
    "        \n",
    "            self.weight2_merge = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use,2), requires_grad = True ) \n",
    "            self.weight2_merge.data.uniform_(1.0, 1.0) # all 1\n",
    "            # Normalization\n",
    "            self.weight2_merge_normalized_fwd = torch.sqrt(self.weight2_merge[:,0]**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge[:,0]**2)) \n",
    "            self.weight2_merge_normalized_bwd  = torch.sqrt(self.weight2_merge[:,1]**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge[:,1]**2))\n",
    "        \n",
    "        if self.param.attention_type== 4: # uni-directional --> N weights\n",
    "            self.weight1_merge = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use),requires_grad = True )\n",
    "            self.weight1_merge.data.uniform_(1.0, 1.0) # all 1\n",
    "            # Normalization\n",
    "            self.weight1_merge_normalized  = torch.sqrt(self.weight1_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge**2))\n",
    "        \n",
    "            self.weight2_merge = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use),requires_grad = True )\n",
    "            self.weight2_merge.data.uniform_(1.0, 1.0) # all 1\n",
    "            # Normalization\n",
    "            self.weight2_merge_normalized  = torch.sqrt(self.weight2_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge**2))\n",
    "        \n",
    "        \n",
    "        # Parameters for normalization (mean and variance)\n",
    "        # User 1\n",
    "        self.mean1_batch = torch.zeros(self.param.N_channel_use) \n",
    "        self.std1_batch = torch.ones(self.param.N_channel_use)\n",
    "        self.mean1_saved = torch.zeros(self.param.N_channel_use)\n",
    "        self.std1_saved = torch.ones(self.param.N_channel_use)\n",
    "        # User 2\n",
    "        self.mean2_batch = torch.zeros(self.param.N_channel_use) \n",
    "        self.std2_batch = torch.ones(self.param.N_channel_use)\n",
    "        self.mean2_saved = torch.zeros(self.param.N_channel_use)\n",
    "        self.std2_saved = torch.ones(self.param.N_channel_use)\n",
    "        self.normalization_with_saved_data = False   # True: inference with saved mean/var, False: calculate mean/var\n",
    "\n",
    "    def decoder_activation(self, inputs):\n",
    "        if self.param.output_type == 'bit_vector':\n",
    "            return torch.sigmoid(inputs) # training with binary cross entropy\n",
    "        elif self.param.output_type == 'one_hot_vector':\n",
    "            return inputs # Note. softmax function is applied in \"F.cross_entropy\" function\n",
    "    \n",
    "    # Convert `bit vector' to 'one-hot vector'\n",
    "    def one_hot(self, bit_vec):\n",
    "#         bit_vec = bit_vec.view(parameter.batch_size, parameter.N_bits)\n",
    "        bit_vec = bit_vec.view(parameter.batch_size, -1)\n",
    "        N_batch = bit_vec.size(0) # batch_size\n",
    "        N_bits = bit_vec.size(1)  # N_bits=K\n",
    "\n",
    "        ind = torch.arange(0,N_bits).repeat(N_batch,1) \n",
    "        ind = ind.to(device)\n",
    "        ind_vec = torch.sum( torch.mul(bit_vec, 2**ind), axis=1 ).long()\n",
    "        bit_onehot = torch.zeros((N_batch, 2**N_bits), dtype=int)\n",
    "        for ii in range(N_batch):\n",
    "            bit_onehot[ii, ind_vec[ii]]=1 # one-hot vector\n",
    "        return bit_onehot \n",
    "        \n",
    "    def normalization(self, inputs, t_idx, user_idx):\n",
    "        if self.training: # During training\n",
    "            mean_batch = torch.mean(inputs)\n",
    "            std_batch  = torch.std(inputs)\n",
    "            outputs   = (inputs - mean_batch)/std_batch\n",
    "        else: \n",
    "            if self.normalization_with_saved_data: # During inference\n",
    "                if user_idx==1:\n",
    "                    outputs   = (inputs - self.mean1_saved[t_idx])/self.std1_saved[t_idx]\n",
    "                elif user_idx==2:\n",
    "                    outputs   = (inputs - self.mean2_saved[t_idx])/self.std2_saved[t_idx]\n",
    "            else: \n",
    "                # During validation\n",
    "                mean_batch = torch.mean(inputs)\n",
    "                std_batch  = torch.std(inputs)\n",
    "                outputs   = (inputs - mean_batch)/std_batch\n",
    "                # calculate mean/var after training\n",
    "                if user_idx==1:\n",
    "                    self.mean1_batch[t_idx] = mean_batch\n",
    "                    self.std1_batch[t_idx] = std_batch\n",
    "                elif user_idx==2:\n",
    "                    self.mean2_batch[t_idx] = mean_batch\n",
    "                    self.std2_batch[t_idx] = std_batch\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, b1, b2, noise1, noise2):\n",
    "\n",
    "        # Normalize power weights\n",
    "        self.weight_power1_normalized  = torch.sqrt(self.weight_power1**2 *(self.param.N_channel_use)/torch.sum(self.weight_power1**2))\n",
    "        self.weight_power2_normalized  = torch.sqrt(self.weight_power2**2 *(self.param.N_channel_use)/torch.sum(self.weight_power2**2))\n",
    "        \n",
    "        # Encoder input\n",
    "        if self.param.input_type == 'bit_vector':\n",
    "            I1 = b1 \n",
    "            I2 = b2 \n",
    "        elif self.param.input_type == 'one_hot_vector':\n",
    "            I1 = self.one_hot(b1).to(device)\n",
    "            I2 = self.one_hot(b2).to(device)\n",
    "        \n",
    "        for t in range(self.param.N_channel_use): # timesteps\n",
    "            # Encoder\n",
    "            if t == 0: # 1st timestep\n",
    "                input1_total        = torch.cat([I1.view(self.param.batch_size, 1, self.num_input1), \n",
    "                                               torch.zeros((self.param.batch_size, 1, self.num_D+1)).to(device)], dim=2) \n",
    "                ### input1_total   -- (batch, 1, num_input + num_D + 1) \n",
    "                x1_t_after_RNN, s1_t_hidden  = self.encoder1_RNN(input1_total)\n",
    "                ### x1_t_after_RNN -- (batch, 1, hidden)\n",
    "                ### s1_t_hidden    -- (layers, batch, hidden)\n",
    "                x1_t_tilde =   torch.tanh(self.encoder1_linear(x1_t_after_RNN))   \n",
    "                \n",
    "                input2_total        = torch.cat([I2.view(self.param.batch_size, 1, self.num_input2), \n",
    "                                               torch.zeros((self.param.batch_size, 1, self.num_D+1)).to(device)], dim=2) \n",
    "                x2_t_after_RNN, s2_t_hidden  = self.encoder2_RNN(input2_total)\n",
    "                x2_t_tilde =   torch.tanh(self.encoder2_linear(x2_t_after_RNN))   \n",
    "                \n",
    "                \n",
    "            else: # 2nd-Nth timestep\n",
    "                if self.param.decoder_info == 'None':\n",
    "                    input1_total        = torch.cat([I1.view(self.param.batch_size, 1, self.num_input1), y1_t], dim=2) \n",
    "                    input2_total        = torch.cat([I2.view(self.param.batch_size, 1, self.num_input2), y2_t], dim=2) \n",
    "                else:\n",
    "                    input1_total        = torch.cat([I1.view(self.param.batch_size, 1, self.num_input1), y1_t, D1_tmp], dim=2) \n",
    "                    input2_total        = torch.cat([I2.view(self.param.batch_size, 1, self.num_input2), y2_t, D2_tmp], dim=2)\n",
    "                \n",
    "                x1_t_after_RNN, s1_t_hidden  = self.encoder1_RNN(input1_total, s1_t_hidden)\n",
    "                x1_t_tilde =   torch.tanh(self.encoder1_linear(x1_t_after_RNN))\n",
    "                \n",
    "                \n",
    "                x2_t_after_RNN, s2_t_hidden  = self.encoder2_RNN(input2_total, s2_t_hidden)\n",
    "                x2_t_tilde =   torch.tanh(self.encoder2_linear(x2_t_after_RNN))\n",
    "            \n",
    "            # Power control layer: 1. Normalization, 2. Power allocation\n",
    "            x1_t_norm = self.normalization(x1_t_tilde, t, 1).view(self.param.batch_size, 1, 1)\n",
    "            x1_t  = x1_t_norm * self.weight_power1_normalized[t] \n",
    "            x2_t_norm = self.normalization(x2_t_tilde, t, 2).view(self.param.batch_size, 1, 1)\n",
    "            x2_t  = x2_t_norm * self.weight_power2_normalized[t] \n",
    "            \n",
    "            # Forward transmission (from User 1 to 2)\n",
    "            y2_t = x1_t + noise1[:,t,:].view(self.param.batch_size, 1, 1)\n",
    "            \n",
    "            # Backward transmission (from User 2 to 1)\n",
    "            y1_t = x2_t + noise2[:,t,:].view(self.param.batch_size, 1, 1)\n",
    "            \n",
    "            # Concatenate values along time t\n",
    "            if t == 0:\n",
    "#                 x1_norm_total = x1_t_norm\n",
    "                x1_total = x1_t\n",
    "                x2_total = x2_t\n",
    "                y1_total = y1_t\n",
    "                y2_total = y2_t\n",
    "            else:\n",
    "#                 x_norm_total = torch.cat([x_norm_total, x_t_norm], dim=1) \n",
    "                x1_total = torch.cat([x1_total, x1_t ], dim = 1) # In the end, (batch, N, 1)\n",
    "                x2_total = torch.cat([x2_total, x2_t ], dim = 1)\n",
    "                y1_total = torch.cat([y1_total, y1_t ], dim = 1) \n",
    "                y2_total = torch.cat([y2_total, y2_t ], dim = 1) \n",
    "            \n",
    "            # Encoder info updates\n",
    "            if self.param.encoder_info == 'tran_symbol':\n",
    "                E1_tmp = x1_t # (batch,1,1)\n",
    "                E2_tmp = x2_t # (batch,1,1)\n",
    "            elif self.param.encoder_info == 'state_vector':\n",
    "                E1_tmp = x1_t_after_RNN # (batch, 1, hidden) \n",
    "                E2_tmp = x2_t_after_RNN # (batch, 1, hidden) \n",
    "#                 E1_tmp = s1_t_hidden[-1].view(self.param.batch_size, 1, -1) # (batch, 1, hidden) # only last layer\n",
    "#                 E2_tmp = s2_t_hidden[-1].view(self.param.batch_size, 1, -1) # (batch, 1, hidden)\n",
    "            elif self.param.encoder_info == 'None':\n",
    "                E1_tmp = None\n",
    "                E2_tmp = None\n",
    "            \n",
    "            ########################################\n",
    "            # Immediate Decoding\n",
    "            if self.param.decoder_info != 'None': \n",
    "                # Encoder uses decoder info\n",
    "                # --> There is connection from decoder to encoder\n",
    "                # --> Immediate decoding is needed!\n",
    "                if self.param.encoder_info == 'None': # Decoder does not use encoder info\n",
    "                    decoder_input1 = torch.cat([I1.view(self.param.batch_size, 1, self.num_input1), y1_t], dim=2) # (batch, 1, num_input + 1)\n",
    "                    decoder_input2 = torch.cat([I2.view(self.param.batch_size, 1, self.num_input2), y2_t], dim=2) # (batch, 1, num_input + 1)\n",
    "                else:\n",
    "                    decoder_input1 = torch.cat([I1.view(self.param.batch_size, 1, self.num_input1), y1_t, E1_tmp], dim=2) # (batch, 1, num_input + 1 + num_E)\n",
    "                    decoder_input2 = torch.cat([I2.view(self.param.batch_size, 1, self.num_input2), y2_t, E2_tmp], dim=2)\n",
    "    \n",
    "                ### decoder_input1: (batch, 1, num_input + 1 + num_E) -- batch, input seq, input size\n",
    "                r1_t_last, r1_t_hidden  = self.decoder1_RNN(decoder_input1)\n",
    "                ### r1_t_last   -- (batch, 1(=input seq), hidden)\n",
    "                ### r1_t_hidden -- (layer, batch, hidden)\n",
    "                r2_t_last, r2_t_hidden  = self.decoder2_RNN(decoder_input2)\n",
    "                \n",
    "                if self.param.decoder_info == 'state_vector': # No output calculation required\n",
    "                    D1_tmp = r1_t_last\n",
    "                    D2_tmp = r2_t_last\n",
    "                    if t==0:\n",
    "                        r1_hidden = r1_t_last\n",
    "                        r2_hidden = r2_t_last\n",
    "                    else:\n",
    "                        r1_hidden = torch.cat([r1_hidden, r1_t_last], dim=1) # Finally, (batch, N, hidden)\n",
    "                        r2_hidden = torch.cat([r2_hidden, r2_t_last], dim=1)\n",
    "                        \n",
    "#                 if self.decoder_info == 'bit_estimate':\n",
    "                \n",
    "#                     output1     = self.decoder_activation(self.decoder1_linear(z1_t_after_RNN)) # (batch,1,num_output)\n",
    "#                     output1_last = output1.view(self.param.batch_size,-1,1) # (batch, num_output, 1)\n",
    "                \n",
    "        # Decoder do inference after N transmission are conducted!   \n",
    "        if self.param.decoder_info == 'state_vector':\n",
    "            # Only Uni-directinoal is possible\n",
    "            \n",
    "            # Normalize attention weights (Uni-directional attention weights)\n",
    "            self.weight1_merge_normalized  = torch.sqrt(self.weight1_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge**2)) \n",
    "            self.weight2_merge_normalized  = torch.sqrt(self.weight2_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge**2)) \n",
    "            \n",
    "            # Multiply attention weights\n",
    "            r1_merge = torch.tensordot(r1_hidden, self.weight1_merge_normalized, dims=([1], [0])) # (batch, hidden)\n",
    "            output1 = self.decoder_activation(self.decoder1_linear(r1_merge)) \n",
    "            output1_last = output1.view(self.param.batch_size,-1,1) # (batch, num_output, 1)\n",
    "\n",
    "            r2_merge = torch.tensordot(r2_hidden, self.weight2_merge_normalized, dims=([1], [0])) # (batch, hidden)\n",
    "            output2 = self.decoder_activation(self.decoder2_linear(r2_merge)) \n",
    "            output2_last = output2.view(self.param.batch_size,-1,1) # (batch, num_output, 1)\n",
    "    \n",
    "    \n",
    "        # Non-immediate Decoding\n",
    "        if self.param.decoder_info == 'None': # No connection from decoder to encoder\n",
    "            # Normalize attention weights\n",
    "            if parameter.attention_type== 4:\n",
    "                self.weight1_merge_normalized  = torch.sqrt(self.weight1_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge**2)) \n",
    "                self.weight2_merge_normalized  = torch.sqrt(self.weight2_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge**2)) \n",
    "            if parameter.attention_type== 5:\n",
    "                self.weight1_merge_normalized_fwd  = torch.sqrt(self.weight1_merge[:,0]**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge[:,0]**2)) # 30\n",
    "                self.weight1_merge_normalized_bwd  = torch.sqrt(self.weight1_merge[:,1]**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge[:,1]**2))\n",
    "                self.weight2_merge_normalized_fwd  = torch.sqrt(self.weight2_merge[:,0]**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge[:,0]**2)) # 30\n",
    "                self.weight2_merge_normalized_bwd  = torch.sqrt(self.weight2_merge[:,1]**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge[:,1]**2))\n",
    "\n",
    "                \n",
    "            I1_tmp = I1.view(self.param.batch_size, 1, self.num_input1)\n",
    "            I1_copy = I1_tmp.repeat(1, self.param.N_channel_use, 1) # (batch, N, K1)\n",
    "            I2_tmp = I2.view(self.param.batch_size, 1, self.num_input2)\n",
    "            I2_copy = I2_tmp.repeat(1, self.param.N_channel_use, 1) # (batch, N, K2)\n",
    "            if self.param.encoder_info == 'None':\n",
    "                decoder1_input = torch.cat([I1_copy, y1_total], dim=2) # (batch, N, K1+1)\n",
    "                decoder2_input = torch.cat([I2_copy, y2_total], dim=2) # (batch, N, K2+1)\n",
    "            elif self.param.encoder_info == 'tran_symbol':\n",
    "                decoder1_input = torch.cat([I1_copy, x1_total, y1_total], dim=2) # (batch, N, K1+2)\n",
    "                decoder2_input = torch.cat([I2_copy, x2_total, y2_total], dim=2) # (batch, N, K2+2)\n",
    "            \n",
    "            r1_hidden, _  = self.decoder1_RNN(decoder1_input) # (batch, N, bi*hidden_size)\n",
    "            r2_hidden, _  = self.decoder2_RNN(decoder2_input) # (batch, N, bi*hidden_size)\n",
    "\n",
    "\n",
    "    #         # Option 1. Only the N-th timestep\n",
    "    #         if parameter.attention_type== 1:\n",
    "    #             output     = self.decoder_activation(self.decoder_linear(r_hidden)) #(batch,N,bi*hidden)-->(batch,N,num_output)\n",
    "    #             output_last = output[:,-1,:].view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "    #         # Option 2. Merge the \"last\" outputs of forward/backward RNN\n",
    "    #         if parameter.attention_type== 2:\n",
    "    #             r_backward = r_hidden[:,0,self.param.decoder_N_neurons:] # Output at the 1st timestep of reverse RNN \n",
    "    #             r_forward = r_hidden[:,-1,:self.param.decoder_N_neurons] # Output at the N-th timestep of forward RNN\n",
    "    #             r_concat = torch.cat([r_backward, r_forward ], dim = 1) \n",
    "    #             output = self.decoder_activation(self.decoder_linear(r_concat)) # (batch,num_output)\n",
    "    #             output_last = output.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "    #         # Option 3. Sum over all timesteps\n",
    "    #         if parameter.attention_type== 3:\n",
    "    #             output     = self.decoder_activation(self.decoder_linear(r_hidden)) \n",
    "    #             output_last = torch.sum(output, dim=1).view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "            # Option 4. Attention mechanism (N weights)\n",
    "            if parameter.attention_type== 4:\n",
    "                r1_concat = torch.tensordot(r1_hidden, self.weight1_merge_normalized, dims=([1], [0])) # (batch, hidden_size)\n",
    "                output1 = self.decoder_activation(self.decoder1_linear(r1_concat)) \n",
    "                output1_last = output1.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "                \n",
    "                r2_concat = torch.tensordot(r2_hidden, self.weight2_merge_normalized, dims=([1], [0])) # (batch, hidden_size)\n",
    "                output2 = self.decoder_activation(self.decoder2_linear(r2_concat)) \n",
    "                output2_last = output2.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "            # Option 5. Attention mechanism (2N weights) for forward/backward\n",
    "            if parameter.attention_type== 5:\n",
    "                r1_hidden_forward = r1_hidden[:,:,:self.param.decoder_N_neurons]  # (batch,num_output,hidden_size)\n",
    "                r1_hidden_backward = r1_hidden[:,:,self.param.decoder_N_neurons:] # (batch,num_output,hidden_size)\n",
    "                r1_forward_weighted_sum = torch.tensordot(r1_hidden_forward, self.weight1_merge_normalized_fwd, dims=([1], [0]))  # (batch,hidden_size)\n",
    "                r1_backward_weighted_sum = torch.tensordot(r1_hidden_backward, self.weight1_merge_normalized_bwd, dims=([1], [0]))         # (batch,hidden_size)\n",
    "                r1_concat = torch.cat([r1_forward_weighted_sum, r1_backward_weighted_sum], dim = 1) \n",
    "                output1 = self.decoder_activation(self.decoder1_linear(r1_concat)) \n",
    "                output1_last = output1.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "                r2_hidden_forward = r2_hidden[:,:,:self.param.decoder_N_neurons]  # (batch,num_output,hidden_size)\n",
    "                r2_hidden_backward = r2_hidden[:,:,self.param.decoder_N_neurons:] # (batch,num_output,hidden_size)\n",
    "                r2_forward_weighted_sum = torch.tensordot(r2_hidden_forward, self.weight2_merge_normalized_fwd, dims=([1], [0]))  # (batch,hidden_size)\n",
    "                r2_backward_weighted_sum = torch.tensordot(r2_hidden_backward, self.weight2_merge_normalized_bwd, dims=([1], [0]))         # (batch,hidden_size)\n",
    "                r2_concat = torch.cat([r2_forward_weighted_sum, r2_backward_weighted_sum], dim = 1) \n",
    "                output2 = self.decoder_activation(self.decoder2_linear(r2_concat)) \n",
    "                output2_last = output2.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "            \n",
    "        self.x1 = x1_total                    # (batch,N,1)\n",
    "        self.x2 = x2_total                    # (batch,N,1)\n",
    "        \n",
    "        return output1_last, output2_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the `bit vector' with (batch,K,1) to 'one hot vector' with (batch,2^K)\n",
    "def one_hot(bit_vec):\n",
    "    bit_vec = bit_vec.squeeze(-1)  # (batch, K)\n",
    "    N_batch = bit_vec.size(0) \n",
    "    N_bits = bit_vec.size(1)\n",
    "\n",
    "    ind = torch.arange(0,N_bits).repeat(N_batch,1) # (batch, K)\n",
    "    ind = ind.to(device)\n",
    "    ind_vec = torch.sum( torch.mul(bit_vec,2**ind), axis=1).long() # batch\n",
    "    b_onehot = torch.zeros((N_batch, 2**N_bits), dtype=int)\n",
    "    for ii in range(N_batch):\n",
    "        b_onehot[ii, ind_vec[ii]]=1 # one-hot vector\n",
    "    return b_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test_RNN(N_test): \n",
    "\n",
    "    # Generate test data\n",
    "    bit1_test     = torch.randint(0, 2, (N_test, parameter.K1, 1)) \n",
    "    bit2_test     = torch.randint(0, 2, (N_test, parameter.K2, 1)) \n",
    "    noise1_test  = sigma1*torch.randn((N_test, parameter.N_channel_use,1))\n",
    "    noise2_test   = sigma2*torch.randn((N_test, parameter.N_channel_use,1))\n",
    "    \n",
    "    model.eval() # model.training() becomes False\n",
    "    N_iter = (N_test//parameter.batch_size) # N_test should be multiple of batch_size\n",
    "    ber1=0\n",
    "    bler1=0\n",
    "    ber2=0\n",
    "    bler2=0\n",
    "    power1_acc = np.zeros((parameter.batch_size, parameter.N_channel_use ,1))\n",
    "    power2_acc = np.zeros((parameter.batch_size, parameter.N_channel_use ,1))\n",
    "    with torch.no_grad():\n",
    "        for i in range(N_iter):\n",
    "            bit1 = bit1_test[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.K1,1) # batch, K,1\n",
    "            bit2 = bit2_test[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.K2,1)\n",
    "            noise1 = noise1_test[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1) # batch, N,1\n",
    "            noise2 = noise2_test[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1) # batch, N,1\n",
    "\n",
    "            bit1 = bit1.to(device)\n",
    "            bit2 = bit2.to(device)\n",
    "            noise1 = noise1.to(device)\n",
    "            noise2 = noise2.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            X2_hat, X1_hat = model(bit1, bit2, noise1, noise2)\n",
    "\n",
    "            if parameter.output_type == 'bit_vector':\n",
    "                ber1_tmp, bler1_tmp = error_rate_bitvector(X1_hat.cpu(), bit1.cpu())\n",
    "                ber2_tmp, bler2_tmp = error_rate_bitvector(X2_hat.cpu(), bit2.cpu())\n",
    "            elif parameter.output_type == 'one_hot_vector':\n",
    "                ber1_tmp, bler1_tmp = error_rate_onehot(X1_hat.cpu(), bit1.cpu())\n",
    "                ber2_tmp, bler2_tmp = error_rate_onehot(X2_hat.cpu(), bit2.cpu())\n",
    "                \n",
    "            ber1 = ber1 + ber1_tmp\n",
    "            ber2 = ber2 + ber2_tmp\n",
    "            bler1 = bler1 + bler1_tmp\n",
    "            bler2 = bler2 + bler2_tmp\n",
    "            \n",
    "            # Power\n",
    "            signal1 = model.x1.cpu().detach().numpy()\n",
    "            power1_acc += signal1**2 \n",
    "            signal2 = model.x2.cpu().detach().numpy()\n",
    "            power2_acc += signal2**2 \n",
    "            \n",
    "        ber1  = ber1/N_iter\n",
    "        ber2  = ber2/N_iter\n",
    "        bler1 = bler1/N_iter\n",
    "        bler2 = bler2/N_iter\n",
    "        power1_avg = power1_acc/N_iter\n",
    "        power2_avg = power2_acc/N_iter\n",
    "\n",
    "    return ber1, ber2, bler1, bler2, power1_avg, power2_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twoway_coding(\n",
      "  (encoder1_RNN): GRU(7, 50, num_layers=2, batch_first=True)\n",
      "  (encoder1_linear): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (encoder2_RNN): GRU(7, 50, num_layers=2, batch_first=True)\n",
      "  (encoder2_linear): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (decoder1_RNN): GRU(8, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (decoder1_linear): Linear(in_features=100, out_features=64, bias=True)\n",
      "  (decoder2_RNN): GRU(8, 50, num_layers=2, batch_first=True, bidirectional=True)\n",
      "  (decoder2_linear): Linear(in_features=100, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    model = Twoway_coding(parameter).to(device)\n",
    "else:\n",
    "    model = Twoway_coding(parameter)\n",
    "\n",
    "print(model)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=parameter.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training \n",
      "weight_power1:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "weight_power2:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "weight1_merge_fwd:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "weight1_merge_bwd:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "weight2_merge_fwd:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "weight2_merge_bwd:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n",
      "Epoch: 0, Iter: 0 out of 400, Loss: 10.9219\n",
      "Epoch: 0, Iter: 100 out of 400, Loss: 3.1653\n",
      "Epoch: 0, Iter: 200 out of 400, Loss: 2.8243\n",
      "Epoch: 0, Iter: 300 out of 400, Loss: 2.5596\n",
      "Summary: Epoch: 0, lr: 0.01, Average loss: 3.1404\n",
      "weight_power1:  [1.606 1.272 1.098 1.056 1.025 0.938 0.983 0.929 0.865 0.894 0.885 0.846\n",
      " 0.864 0.868 0.778 0.731 0.9   1.108]\n",
      "weight_power2:  [1.65  1.17  0.972 1.003 1.038 0.983 0.943 0.931 0.895 0.819 0.771 0.767\n",
      " 0.783 0.787 0.779 0.847 1.048 1.36 ]\n",
      "weight1_merge_fwd:  [0.896 1.137 1.351 1.476 1.493 1.433 1.457 1.425 1.234 0.917 0.651 0.473\n",
      " 0.355 0.278 0.226 0.192 0.191 0.357]\n",
      "weight1_merge_bwd:  [0.538 0.594 0.771 0.999 1.196 1.354 1.414 1.327 1.189 1.072 0.978 0.893\n",
      " 0.82  0.766 0.923 0.869 0.851 0.912]\n",
      "weight2_merge_fwd:  [1.358e+00 1.656e+00 2.148e+00 2.446e+00 1.455e+00 7.860e-01 2.260e-01\n",
      " 4.000e-03 0.000e+00 3.000e-03 2.300e-02 6.400e-02 6.800e-02 1.700e-02\n",
      " 9.000e-03 5.000e-03 2.000e-03 1.490e-01]\n",
      "weight2_merge_bwd:  [0.09  0.018 0.004 0.004 0.011 0.1   0.61  1.086 1.515 1.606 1.407 1.123\n",
      " 0.894 0.604 0.639 1.097 1.48  1.83 ]\n",
      "\n",
      "Ber1:   0.24122832715511322\n",
      "Ber2:   0.0\n",
      "Bler1:  0.6246500015258789\n",
      "Bler2:  0.0\n",
      "\n",
      "Epoch: 1, Iter: 0 out of 400, Loss: 2.1606\n",
      "Epoch: 1, Iter: 100 out of 400, Loss: 2.1012\n",
      "Epoch: 1, Iter: 200 out of 400, Loss: 2.0373\n",
      "Epoch: 1, Iter: 300 out of 400, Loss: 2.0039\n",
      "Summary: Epoch: 1, lr: 0.0095, Average loss: 2.0726\n",
      "weight_power1:  [1.39  1.336 1.069 0.978 0.968 0.955 0.96  0.951 0.942 0.929 0.909 0.909\n",
      " 0.901 0.902 0.897 0.861 0.835 1.111]\n",
      "weight_power2:  [1.517 0.998 0.758 1.004 0.859 1.007 0.91  0.954 0.932 0.957 0.942 0.973\n",
      " 0.986 0.959 1.018 0.912 0.928 1.178]\n",
      "weight1_merge_fwd:  [0.879 1.149 1.378 1.492 1.511 1.451 1.456 1.389 1.194 0.901 0.652 0.477\n",
      " 0.358 0.278 0.225 0.192 0.19  0.351]\n",
      "weight1_merge_bwd:  [0.524 0.589 0.784 1.015 1.233 1.423 1.512 1.428 1.292 1.145 1.017 0.897\n",
      " 0.776 0.654 0.727 0.623 0.638 0.831]\n",
      "weight2_merge_fwd:  [1.133 1.456 2.215 2.733 1.389 0.462 0.005 0.004 0.133 0.23  0.019 0.009\n",
      " 0.006 0.04  0.028 0.007 0.005 0.005]\n",
      "weight2_merge_bwd:  [0.013 0.016 0.01  0.014 0.021 0.015 0.366 0.946 1.564 2.074 1.376 0.858\n",
      " 0.441 0.211 0.673 1.193 1.241 1.984]\n",
      "\n",
      "Ber1:   0.24308833479881287\n",
      "Ber2:   2.833333201124333e-05\n",
      "Bler1:  0.5777699947357178\n",
      "Bler2:  5.999999848427251e-05\n",
      "\n",
      "Epoch: 2, Iter: 0 out of 400, Loss: 2.0303\n",
      "Epoch: 2, Iter: 100 out of 400, Loss: 2.5404\n",
      "Epoch: 2, Iter: 200 out of 400, Loss: 1.9631\n",
      "Epoch: 2, Iter: 300 out of 400, Loss: 1.8937\n",
      "Summary: Epoch: 2, lr: 0.009025, Average loss: 1.9818\n",
      "weight_power1:  [1.081 1.03  1.046 1.04  1.028 1.029 1.04  1.009 1.008 0.996 0.988 0.976\n",
      " 0.971 0.959 0.948 0.957 0.932 0.944]\n",
      "weight_power2:  [1.552 0.849 0.937 0.937 0.884 0.935 0.964 0.94  0.966 0.991 0.994 0.982\n",
      " 0.984 1.021 0.977 0.954 0.972 0.978]\n",
      "weight1_merge_fwd:  [0.892 1.239 1.596 1.74  1.804 1.696 1.467 1.076 0.699 0.38  0.145 0.019\n",
      " 0.021 0.002 0.03  0.011 0.014 0.03 ]\n",
      "weight1_merge_bwd:  [0.203 0.218 0.495 0.782 1.139 1.53  1.809 1.794 1.591 1.31  1.041 0.803\n",
      " 0.572 0.265 0.175 0.    0.    0.722]\n",
      "weight2_merge_fwd:  [1.043e+00 1.397e+00 2.244e+00 2.829e+00 1.303e+00 3.100e-01 2.500e-02\n",
      " 2.700e-02 2.080e-01 2.540e-01 5.000e-03 2.000e-03 4.000e-03 1.180e-01\n",
      " 5.800e-02 1.000e-03 1.300e-02 1.200e-02]\n",
      "weight2_merge_bwd:  [0.006 0.019 0.009 0.006 0.004 0.004 0.246 0.864 1.546 2.208 1.382 0.782\n",
      " 0.212 0.076 0.74  1.38  1.098 1.923]\n",
      "\n",
      "Ber1:   0.22671666741371155\n",
      "Ber2:   2.6666666599339806e-05\n",
      "Bler1:  0.5455099940299988\n",
      "Bler2:  4.999999873689376e-05\n",
      "\n",
      "Epoch: 3, Iter: 0 out of 400, Loss: 1.8698\n",
      "Epoch: 3, Iter: 100 out of 400, Loss: 1.7821\n",
      "Epoch: 3, Iter: 200 out of 400, Loss: 1.7816\n",
      "Epoch: 3, Iter: 300 out of 400, Loss: 1.8023\n",
      "Summary: Epoch: 3, lr: 0.00857375, Average loss: 1.8057\n",
      "weight_power1:  [0.991 0.991 1.007 1.001 1.008 1.011 1.011 1.013 1.005 0.998 1.002 0.999\n",
      " 0.998 0.99  0.998 1.01  0.997 0.97 ]\n",
      "weight_power2:  [1.462 0.712 0.987 1.013 0.939 0.972 1.017 0.962 0.975 1.007 0.973 0.993\n",
      " 1.    0.998 0.98  0.97  1.008 0.873]\n",
      "weight1_merge_fwd:  [8.880e-01 1.284e+00 1.620e+00 1.740e+00 1.799e+00 1.665e+00 1.436e+00\n",
      " 1.066e+00 7.130e-01 4.050e-01 1.690e-01 2.900e-02 1.700e-02 1.000e-03\n",
      " 2.800e-02 2.000e-02 2.500e-02 3.300e-02]\n",
      "weight1_merge_bwd:  [0.193 0.231 0.505 0.781 1.138 1.529 1.829 1.815 1.595 1.311 1.041 0.785\n",
      " 0.524 0.187 0.071 0.    0.    0.704]\n",
      "weight2_merge_fwd:  [1.002e+00 1.382e+00 2.292e+00 2.850e+00 1.242e+00 2.330e-01 2.000e-03\n",
      " 1.000e-03 2.060e-01 2.340e-01 1.200e-02 3.000e-03 2.000e-03 7.800e-02\n",
      " 9.800e-02 2.000e-03 7.000e-03 3.000e-03]\n",
      "weight2_merge_bwd:  [9.000e-03 5.000e-03 6.000e-03 1.600e-02 1.000e-03 1.100e-02 1.880e-01\n",
      " 8.340e-01 1.523e+00 2.234e+00 1.370e+00 7.600e-01 1.330e-01 3.800e-02\n",
      " 7.380e-01 1.443e+00 1.141e+00 1.886e+00]\n",
      "\n",
      "Ber1:   0.22963500022888184\n",
      "Ber2:   2.166666672565043e-05\n",
      "Bler1:  0.5335299968719482\n",
      "Bler2:  3.9999998989515007e-05\n",
      "\n",
      "Epoch: 4, Iter: 0 out of 400, Loss: 1.7976\n",
      "Epoch: 4, Iter: 100 out of 400, Loss: 1.7768\n",
      "Epoch: 4, Iter: 200 out of 400, Loss: 1.7657\n",
      "Epoch: 4, Iter: 300 out of 400, Loss: 1.7838\n",
      "Summary: Epoch: 4, lr: 0.0081450625, Average loss: 1.7838\n",
      "weight_power1:  [0.978 0.987 0.998 1.002 1.008 1.013 1.008 1.012 1.005 1.002 1.011 1.004\n",
      " 1.002 0.997 1.007 0.992 0.992 0.98 ]\n",
      "weight_power2:  [1.39  0.657 0.994 1.033 0.954 0.989 1.057 0.992 0.988 1.029 0.982 1.\n",
      " 1.013 1.004 0.989 0.974 1.01  0.782]\n",
      "weight1_merge_fwd:  [0.889 1.386 1.677 1.745 1.792 1.638 1.357 1.014 0.684 0.383 0.134 0.08\n",
      " 0.055 0.063 0.06  0.069 0.089 0.064]\n",
      "weight1_merge_bwd:  [0.093 0.17  0.448 0.715 1.063 1.449 1.786 1.887 1.661 1.364 1.085 0.803\n",
      " 0.512 0.114 0.012 0.    0.    0.743]\n",
      "weight2_merge_fwd:  [9.470e-01 1.369e+00 2.335e+00 2.884e+00 1.169e+00 1.610e-01 3.000e-03\n",
      " 6.000e-03 1.460e-01 1.790e-01 1.500e-02 7.000e-03 3.000e-03 2.000e-03\n",
      " 1.190e-01 1.800e-02 4.000e-03 2.000e-03]\n",
      "weight2_merge_bwd:  [7.000e-03 1.000e-03 1.100e-02 1.000e-03 9.000e-03 1.300e-02 1.300e-01\n",
      " 7.950e-01 1.488e+00 2.255e+00 1.358e+00 7.390e-01 3.800e-02 1.000e-03\n",
      " 7.040e-01 1.492e+00 1.171e+00 1.888e+00]\n",
      "\n",
      "Ber1:   0.22862832248210907\n",
      "Ber2:   1.8333332263864577e-05\n",
      "Bler1:  0.5367000102996826\n",
      "Bler2:  3.9999998989515007e-05\n",
      "\n",
      "Epoch: 5, Iter: 0 out of 400, Loss: 1.7873\n",
      "Epoch: 5, Iter: 100 out of 400, Loss: 1.7306\n",
      "Epoch: 5, Iter: 200 out of 400, Loss: 1.7434\n",
      "Epoch: 5, Iter: 300 out of 400, Loss: 1.8530\n",
      "Summary: Epoch: 5, lr: 0.007737809374999999, Average loss: 1.7841\n",
      "weight_power1:  [0.974 0.981 0.996 0.991 1.007 1.012 1.008 1.006 1.003 1.001 1.011 1.005\n",
      " 1.011 1.005 1.008 1.003 0.997 0.979]\n",
      "weight_power2:  [1.498 0.774 0.897 0.931 0.901 0.949 1.01  0.981 0.975 1.017 1.    1.019\n",
      " 1.037 1.033 1.016 1.008 1.016 0.738]\n",
      "weight1_merge_fwd:  [1.004e+00 1.714e+00 2.024e+00 1.919e+00 1.740e+00 1.351e+00 9.410e-01\n",
      " 6.470e-01 3.380e-01 3.800e-02 2.700e-02 1.000e-03 1.500e-02 0.000e+00\n",
      " 0.000e+00 2.300e-02 3.700e-02 2.900e-02]\n",
      "weight1_merge_bwd:  [0.048 0.007 0.151 0.46  0.922 1.377 1.871 2.2   1.715 1.333 1.016 0.632\n",
      " 0.214 0.003 0.    0.008 0.01  0.694]\n",
      "weight2_merge_fwd:  [9.030e-01 1.356e+00 2.371e+00 2.903e+00 1.113e+00 1.150e-01 1.000e-03\n",
      " 5.000e-03 1.220e-01 1.240e-01 2.500e-02 4.000e-03 6.000e-03 1.000e-03\n",
      " 1.150e-01 1.000e-02 3.000e-03 8.000e-03]\n",
      "weight2_merge_bwd:  [0.004 0.005 0.003 0.003 0.015 0.016 0.088 0.77  1.469 2.261 1.343 0.718\n",
      " 0.    0.007 0.675 1.524 1.198 1.897]\n",
      "\n",
      "Ber1:   0.23185333609580994\n",
      "Ber2:   2.9999999242136255e-05\n",
      "Bler1:  0.5348699688911438\n",
      "Bler2:  7.999999797903001e-05\n",
      "\n",
      "Epoch: 6, Iter: 0 out of 400, Loss: 1.7850\n",
      "Epoch: 6, Iter: 100 out of 400, Loss: 1.7302\n",
      "Epoch: 6, Iter: 200 out of 400, Loss: 1.7371\n",
      "Epoch: 6, Iter: 300 out of 400, Loss: 1.7492\n",
      "Summary: Epoch: 6, lr: 0.007350918906249998, Average loss: 1.7556\n",
      "weight_power1:  [0.994 1.013 1.007 1.    1.023 1.022 1.009 1.011 0.998 0.999 1.01  0.995\n",
      " 0.991 0.996 0.988 0.977 0.989 0.978]\n",
      "weight_power2:  [1.407 0.696 1.018 0.997 0.92  1.02  1.058 0.991 1.001 1.022 0.999 1.012\n",
      " 1.005 1.014 0.986 0.971 1.014 0.687]\n",
      "weight1_merge_fwd:  [1.017 1.78  2.047 1.904 1.694 1.31  0.919 0.647 0.349 0.053 0.035 0.006\n",
      " 0.01  0.003 0.007 0.027 0.04  0.026]\n",
      "weight1_merge_bwd:  [3.900e-02 1.800e-02 1.560e-01 4.730e-01 9.280e-01 1.372e+00 1.841e+00\n",
      " 2.202e+00 1.726e+00 1.352e+00 1.028e+00 6.220e-01 1.880e-01 0.000e+00\n",
      " 0.000e+00 2.000e-03 3.000e-03 6.930e-01]\n",
      "weight2_merge_fwd:  [8.590e-01 1.357e+00 2.423e+00 2.901e+00 1.052e+00 7.600e-02 1.500e-02\n",
      " 1.000e-03 8.600e-02 8.400e-02 8.000e-03 3.000e-03 1.100e-02 7.000e-03\n",
      " 9.100e-02 1.700e-02 5.000e-03 1.700e-02]\n",
      "weight2_merge_bwd:  [0.005 0.005 0.005 0.02  0.024 0.023 0.041 0.736 1.434 2.232 1.33  0.698\n",
      " 0.011 0.008 0.631 1.568 1.249 1.934]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ber1:   0.23132501542568207\n",
      "Ber2:   3.166666647302918e-05\n",
      "Bler1:  0.5386999845504761\n",
      "Bler2:  4.999999873689376e-05\n",
      "\n",
      "Epoch: 7, Iter: 0 out of 400, Loss: 1.7729\n",
      "Epoch: 7, Iter: 100 out of 400, Loss: 1.7354\n",
      "Epoch: 7, Iter: 200 out of 400, Loss: 1.7283\n",
      "Epoch: 7, Iter: 300 out of 400, Loss: 1.7455\n",
      "Summary: Epoch: 7, lr: 0.006983372960937498, Average loss: 1.7479\n",
      "weight_power1:  [0.99  1.009 1.008 1.003 1.013 1.013 1.004 1.009 0.999 0.999 1.007 0.996\n",
      " 0.993 0.996 0.996 0.989 0.989 0.987]\n",
      "weight_power2:  [1.356 0.67  1.035 0.992 0.914 1.034 1.056 0.997 1.014 1.023 0.998 1.017\n",
      " 1.014 1.024 1.002 0.99  1.025 0.655]\n",
      "weight1_merge_fwd:  [1.023 1.833 2.06  1.889 1.665 1.275 0.901 0.652 0.367 0.075 0.047 0.012\n",
      " 0.007 0.005 0.007 0.032 0.043 0.026]\n",
      "weight1_merge_bwd:  [0.049 0.05  0.174 0.489 0.952 1.398 1.87  2.199 1.691 1.325 1.008 0.606\n",
      " 0.164 0.    0.    0.    0.003 0.71 ]\n",
      "weight2_merge_fwd:  [8.230e-01 1.349e+00 2.456e+00 2.903e+00 1.014e+00 4.500e-02 5.000e-03\n",
      " 5.000e-03 7.100e-02 5.300e-02 4.000e-03 1.000e-02 2.000e-03 5.000e-03\n",
      " 6.000e-02 1.000e-02 9.000e-03 2.000e-03]\n",
      "weight2_merge_bwd:  [1.200e-02 9.000e-03 5.000e-03 1.500e-02 1.200e-02 9.000e-03 2.000e-03\n",
      " 7.150e-01 1.418e+00 2.195e+00 1.319e+00 6.800e-01 0.000e+00 1.000e-03\n",
      " 6.060e-01 1.610e+00 1.304e+00 1.948e+00]\n",
      "\n",
      "Ber1:   0.2316366583108902\n",
      "Ber2:   3.3333333249174757e-06\n",
      "Bler1:  0.5367599725723267\n",
      "Bler2:  9.999999747378752e-06\n",
      "\n",
      "Epoch: 8, Iter: 0 out of 400, Loss: 1.7620\n",
      "Epoch: 8, Iter: 100 out of 400, Loss: 1.7232\n",
      "Epoch: 8, Iter: 200 out of 400, Loss: 1.7260\n",
      "Epoch: 8, Iter: 300 out of 400, Loss: 1.7347\n",
      "Summary: Epoch: 8, lr: 0.006634204312890623, Average loss: 1.7396\n",
      "weight_power1:  [0.978 1.003 1.003 0.995 1.009 1.009 1.003 1.009 1.002 1.003 1.01  1.\n",
      " 0.989 0.998 0.999 0.995 0.997 0.998]\n",
      "weight_power2:  [1.289 0.662 1.048 0.991 0.922 1.066 1.068 1.018 1.043 1.035 1.008 1.026\n",
      " 1.02  1.02  0.998 0.984 1.01  0.603]\n",
      "weight1_merge_fwd:  [1.026 1.889 2.057 1.869 1.637 1.245 0.896 0.668 0.396 0.103 0.062 0.015\n",
      " 0.006 0.005 0.01  0.041 0.05  0.029]\n",
      "weight1_merge_bwd:  [6.300e-02 9.400e-02 2.000e-01 5.220e-01 9.900e-01 1.444e+00 1.927e+00\n",
      " 2.202e+00 1.628e+00 1.266e+00 9.590e-01 5.720e-01 1.350e-01 0.000e+00\n",
      " 0.000e+00 0.000e+00 1.000e-03 7.260e-01]\n",
      "weight2_merge_fwd:  [0.788 1.342 2.485 2.906 0.974 0.005 0.003 0.011 0.052 0.025 0.004 0.011\n",
      " 0.004 0.006 0.037 0.01  0.016 0.006]\n",
      "weight2_merge_bwd:  [4.000e-03 1.000e-02 1.000e-03 1.400e-02 3.000e-03 1.300e-02 9.000e-03\n",
      " 6.870e-01 1.396e+00 2.158e+00 1.308e+00 6.610e-01 8.000e-03 8.000e-03\n",
      " 5.720e-01 1.643e+00 1.367e+00 1.969e+00]\n",
      "\n",
      "Ber1:   0.2322116643190384\n",
      "Ber2:   4.999999873689376e-06\n",
      "Bler1:  0.5366600155830383\n",
      "Bler2:  1.9999999494757503e-05\n",
      "\n",
      "Epoch: 9, Iter: 0 out of 400, Loss: 1.7469\n",
      "Epoch: 9, Iter: 100 out of 400, Loss: 1.7170\n",
      "Epoch: 9, Iter: 200 out of 400, Loss: 1.7245\n",
      "Epoch: 9, Iter: 300 out of 400, Loss: 1.7371\n",
      "Summary: Epoch: 9, lr: 0.006302494097246091, Average loss: 1.7351\n",
      "weight_power1:  [0.981 1.006 1.005 0.997 1.009 1.007 1.003 1.01  1.003 1.006 1.008 0.997\n",
      " 0.99  0.993 1.001 0.994 0.992 0.999]\n",
      "weight_power2:  [1.226 0.657 1.052 0.983 0.932 1.081 1.077 1.021 1.064 1.039 1.017 1.04\n",
      " 1.03  1.032 1.005 0.986 1.    0.564]\n",
      "weight1_merge_fwd:  [1.016 1.93  2.04  1.855 1.613 1.227 0.905 0.695 0.435 0.144 0.089 0.031\n",
      " 0.012 0.015 0.025 0.06  0.065 0.04 ]\n",
      "weight1_merge_bwd:  [6.900e-02 1.350e-01 2.230e-01 5.570e-01 1.028e+00 1.498e+00 2.000e+00\n",
      " 2.198e+00 1.550e+00 1.189e+00 9.030e-01 5.380e-01 1.050e-01 1.000e-03\n",
      " 0.000e+00 3.000e-03 4.000e-03 7.420e-01]\n",
      "weight2_merge_fwd:  [7.450e-01 1.333e+00 2.514e+00 2.909e+00 9.410e-01 4.000e-03 1.000e-02\n",
      " 2.000e-03 3.600e-02 1.000e-02 8.000e-03 5.000e-03 2.000e-03 0.000e+00\n",
      " 1.600e-02 1.200e-02 5.000e-03 9.000e-03]\n",
      "weight2_merge_bwd:  [8.000e-03 1.000e-03 2.000e-03 2.400e-02 2.000e-03 2.000e-03 3.000e-03\n",
      " 6.660e-01 1.383e+00 2.137e+00 1.298e+00 6.420e-01 4.000e-03 1.000e-03\n",
      " 5.450e-01 1.664e+00 1.407e+00 1.983e+00]\n",
      "\n",
      "Ber1:   0.23074665665626526\n",
      "Ber2:   1.4999999621068127e-05\n",
      "Bler1:  0.5332900285720825\n",
      "Bler2:  1.9999999494757503e-05\n",
      "\n",
      "Epoch: 10, Iter: 0 out of 400, Loss: 1.7402\n",
      "Epoch: 10, Iter: 100 out of 400, Loss: 1.7123\n",
      "Epoch: 10, Iter: 200 out of 400, Loss: 1.7225\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epoch = 100\n",
    "clipping_value = 1\n",
    "\n",
    "print('Before training ')\n",
    "print('weight_power1: ', model.weight_power1_normalized.cpu().detach().numpy().round(3))\n",
    "print('weight_power2: ', model.weight_power2_normalized.cpu().detach().numpy().round(3))\n",
    "if parameter.attention_type==4:\n",
    "    print('weight1_merge: ', model.weight1_merge_normalized.cpu().detach().numpy().round(3))\n",
    "    print('weight2_merge: ', model.weight2_merge_normalized.cpu().detach().numpy().round(3))\n",
    "if parameter.attention_type==5:\n",
    "    print('weight1_merge_fwd: ', model.weight1_merge_normalized_fwd.cpu().detach().numpy().round(3))\n",
    "    print('weight1_merge_bwd: ', model.weight1_merge_normalized_bwd.cpu().detach().numpy().round(3))\n",
    "    print('weight2_merge_fwd: ', model.weight2_merge_normalized_fwd.cpu().detach().numpy().round(3))\n",
    "    print('weight2_merge_bwd: ', model.weight2_merge_normalized_bwd.cpu().detach().numpy().round(3))\n",
    "print()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    model.train() # model.training() becomes True\n",
    "    loss_training = 0\n",
    "    \n",
    "    N_iter = (N_train//parameter.batch_size)\n",
    "    for i in range(N_iter):\n",
    "        bit1 = bit1_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.K1,1) \n",
    "        bit2 = bit2_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.K2,1) \n",
    "        noise1 = noise1_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1)\n",
    "        noise2 = noise2_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1)\n",
    "\n",
    "        bit1   = bit1.to(device)\n",
    "        bit2   = bit2.to(device)\n",
    "        noise1 = noise1.to(device)\n",
    "        noise2 = noise2.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        optimizer.zero_grad() \n",
    "        X2_hat, X1_hat = model(bit1, bit2, noise1, noise2)\n",
    "\n",
    "        # Define loss according to output type\n",
    "        if parameter.output_type == 'bit_vector':\n",
    "            bit1 = bit1.type(torch.float32)\n",
    "            bit2 = bit2.type(torch.float32)\n",
    "            loss = F.binary_cross_entropy(X1_hat, bit1) + F.binary_cross_entropy(X2_hat, bit2)\n",
    "        elif parameter.output_type == 'one_hot_vector':\n",
    "            bit1_hot =  one_hot(bit1).view(parameter.batch_size, 2**parameter.K1, 1) # (batch,2^K,1)\n",
    "            bit2_hot =  one_hot(bit2).view(parameter.batch_size, 2**parameter.K2, 1)\n",
    "            loss = (F.cross_entropy(X1_hat.squeeze(-1), torch.argmax(bit1_hot,dim=1).squeeze(-1).to(device)) # (batch,2^K), (batch)\n",
    "                    + F.cross_entropy(X2_hat.squeeze(-1), torch.argmax(bit2_hot,dim=1).squeeze(-1).to(device)))\n",
    "        \n",
    "        # training\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
    "        loss_training += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {}, Iter: {} out of {}, Loss: {:.4f}'.format(epoch, i, N_iter, loss.item()))\n",
    "\n",
    "    # Summary of each epoch\n",
    "    print('Summary: Epoch: {}, lr: {}, Average loss: {:.4f}'.format(epoch, optimizer.param_groups[0]['lr'], loss_training/N_iter) )\n",
    "\n",
    "    scheduler.step() # reduce learning rate\n",
    "    \n",
    "    print('weight_power1: ', model.weight_power1_normalized.cpu().detach().numpy().round(3))\n",
    "    print('weight_power2: ', model.weight_power2_normalized.cpu().detach().numpy().round(3))\n",
    "    if parameter.attention_type==4:\n",
    "        print('weight1_merge: ', model.weight1_merge_normalized.cpu().detach().numpy().round(3))\n",
    "        print('weight2_merge: ', model.weight2_merge_normalized.cpu().detach().numpy().round(3))\n",
    "    if parameter.attention_type==5:\n",
    "        print('weight1_merge_fwd: ', model.weight1_merge_normalized_fwd.cpu().detach().numpy().round(3))\n",
    "        print('weight1_merge_bwd: ', model.weight1_merge_normalized_bwd.cpu().detach().numpy().round(3))\n",
    "        print('weight2_merge_fwd: ', model.weight2_merge_normalized_fwd.cpu().detach().numpy().round(3))\n",
    "        print('weight2_merge_bwd: ', model.weight2_merge_normalized_bwd.cpu().detach().numpy().round(3))\n",
    "    print()\n",
    "    \n",
    "    # Validation\n",
    "    ber1_val, ber2_val, bler1_val, bler2_val, _, _ = test_RNN(N_validation)\n",
    "    print('Ber1:  ', float(ber1_val))\n",
    "    print('Ber2:  ', float(ber2_val))\n",
    "    print('Bler1: ', float(bler1_val))\n",
    "    print('Bler2: ', float(bler2_val))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate mean/var with training data\n",
    "model.eval()   # model.training() becomes False\n",
    "N_iter = N_train//parameter.batch_size\n",
    "mean1_train = torch.zeros(parameter.N_channel_use)\n",
    "std1_train  = torch.zeros(parameter.N_channel_use)\n",
    "mean2_train = torch.zeros(parameter.N_channel_use)\n",
    "std2_train  = torch.zeros(parameter.N_channel_use)\n",
    "mean1_total = 0\n",
    "std1_total = 0\n",
    "mean2_total = 0\n",
    "std2_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(N_iter):\n",
    "        bit1   = bit1_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.K1,1) \n",
    "        bit2   = bit2_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.K2,1) \n",
    "        noise1 = noise1_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1)\n",
    "        noise2 = noise2_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1)\n",
    "\n",
    "        bit1   = bit1.to(device)\n",
    "        bit2   = bit2.to(device)\n",
    "        noise1 = noise1.to(device)\n",
    "        noise2 = noise2.to(device)\n",
    "        \n",
    "        X2_hat, X1_hat = model(bit1, bit2, noise1, noise2)\n",
    "        mean1_total += model.mean1_batch\n",
    "        std1_total  += model.std1_batch\n",
    "        mean2_total += model.mean2_batch\n",
    "        std2_total  += model.std2_batch\n",
    "        if i%100==0: print(i)\n",
    "        \n",
    "mean1_train = mean1_total/N_iter\n",
    "std1_train = std1_total/N_iter\n",
    "mean2_train = mean2_total/N_iter\n",
    "std2_train = std2_total/N_iter\n",
    "print('Mean1: ',mean1_train)\n",
    "print('std1 : ',std1_train)\n",
    "print('Mean2: ',mean2_train)\n",
    "print('std2 : ',std2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Save model\n",
    "# save_results_to = ('saved_model/'+ 'diff_rates/' +\n",
    "#                      'K1_K2_N(3_3_9)/bi_sigmoid/SNR1(1dB)SNR2(30dB)/train(1e9)/')\n",
    "save_results_to = 'saved_model/'+ 'diff_rates/K1_K2_N(6_6_18)/bi_softmax/SNR1(-5dB)SNR2(30dB)/'\n",
    "# save_results_to = 'saved_model/'+ 'diff_rates/K1_K2_N(3_3_9)/bi_sigmoid/SNR1(-10dB)SNR2(-10dB)/'\n",
    "\n",
    "torch.save(model.state_dict(), save_results_to+'model.pth')\n",
    "# np2 = sigma2_train**2\n",
    "# save_results_to = 'saved_model/'+ 'np2_'+ str(np2)+'/'\n",
    "\n",
    "####### Save normalization weights\n",
    "torch.save(mean1_train, save_results_to+'mean1_train.pt')\n",
    "torch.save(std1_train, save_results_to+'std1_train.pt')\n",
    "torch.save(mean2_train, save_results_to+'mean2_train.pt')\n",
    "torch.save(std2_train, save_results_to+'std2_train.pt')\n",
    "\n",
    "save_results_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inference stage\n",
    "# N_inference = int(4e8) \n",
    "# N_small = int(1e5) # In case that N_inference is very large, we divide into small chunks\n",
    "N_inference = int(1e10) \n",
    "N_small = int(1e6) \n",
    "N_iter  = N_inference//N_small\n",
    "\n",
    "model.normalization_with_saved_data = True\n",
    "model.mean1_saved = mean1_train\n",
    "model.std1_saved  = std1_train\n",
    "model.mean2_saved = mean2_train\n",
    "model.std2_saved  = std2_train\n",
    "\n",
    "ber1_sum  = 0\n",
    "bler1_sum = 0\n",
    "ber2_sum  = 0\n",
    "bler2_sum = 0\n",
    "power1_sum = np.zeros((parameter.batch_size, parameter.N_channel_use ,1))\n",
    "power2_sum = np.zeros((parameter.batch_size, parameter.N_channel_use ,1))\n",
    "\n",
    "for ii in range(N_iter):\n",
    "    ber1_tmp, ber2_tmp, bler1_tmp, bler2_tmp, power1_tmp, power2_tmp = test_RNN(N_small)\n",
    "    ber1_sum += ber1_tmp\n",
    "    ber2_sum += ber2_tmp\n",
    "    bler1_sum += bler1_tmp\n",
    "    bler2_sum += bler2_tmp\n",
    "    power1_sum += power1_tmp # (batch, N, 1)\n",
    "    power2_sum += power2_tmp\n",
    "    if ii%100==0: \n",
    "        print('Iter: {} out of {}'.format(ii, N_iter))\n",
    "        print('Ber1:  ', float(ber1_sum/(ii+1)))\n",
    "        print('Ber2:  ', float(ber2_sum/(ii+1)))\n",
    "        print('Bler1: ', float(bler1_sum/(ii+1)))\n",
    "        print('Bler2: ', float(bler2_sum/(ii+1)))\n",
    "        print('Power1: ', round(np.sum(power1_sum)/(parameter.batch_size*(ii+1)),3))\n",
    "        print('Power2: ', round(np.sum(power2_sum)/(parameter.batch_size*(ii+1)),3))\n",
    "\n",
    "ber1_inference  = ber1_sum/N_iter\n",
    "ber2_inference  = ber2_sum/N_iter\n",
    "bler1_inference = bler1_sum/N_iter\n",
    "bler2_inference = bler2_sum/N_iter\n",
    "\n",
    "print()\n",
    "print('Ber1:  ', float(ber1_inference))\n",
    "print('Ber2:  ', float(ber2_inference))\n",
    "print('Bler1: ', float(bler1_inference))\n",
    "print('Bler2: ', float(bler2_inference))\n",
    "print('Power1: ', round(np.sum(power1_sum)/(parameter.batch_size*N_iter),3))\n",
    "print('Power2: ', round(np.sum(power2_sum)/(parameter.batch_size*N_iter),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Iter: {} out of {}'.format(ii, N_iter))\n",
    "print('Ber1:  ', float(ber1_sum/(ii+1)))\n",
    "print('Ber2:  ', float(ber2_sum/(ii+1)))\n",
    "print('Bler1: ', float(bler1_sum/(ii+1)))\n",
    "print('Bler2: ', float(bler2_sum/(ii+1)))\n",
    "print('Power1: ', round(np.sum(power1_sum)/(parameter.batch_size*(ii+1)),3))\n",
    "print('Power2: ', round(np.sum(power2_sum)/(parameter.batch_size*(ii+1)),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saved_model/diff_rates/K1_K2_N(6_6_18)/bi_sigmoid/SNR1(-1dB)SNR2(5dB)/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Recall model\n",
    "# save_results_to = 'saved_model/'+ 'diff_rates/K1_K2_N(3_3_9)/bi_softmax/SNR1(5dB)SNR2(20dB)/'\n",
    "# save_results_to = 'saved_model/'+ 'diff_rates/K1_K2_N(3_3_9)/bi_sigmoid/SNR1(1dB)SNR2(30dB)/train(1e8)/'\n",
    "save_results_to = ('saved_model/'+ 'diff_rates/' +\n",
    "                     'K1_K2_N(3_3_9)/bi_sigmoid/SNR1(1dB)SNR2(20dB)/train(1e8)/')\n",
    "\n",
    "model.load_state_dict(torch.load(save_results_to+'model.pth'))\n",
    "model.eval()\n",
    "\n",
    "\n",
    "####### Load normalization weights\n",
    "mean1_train = torch.load(save_results_to+'mean1_train.pt')\n",
    "std1_train = torch.load(save_results_to+'std1_train.pt')\n",
    "mean2_train = torch.load(save_results_to+'mean2_train.pt')\n",
    "std2_train = torch.load(save_results_to+'std2_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my-torch",
   "language": "python",
   "name": "my-torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
