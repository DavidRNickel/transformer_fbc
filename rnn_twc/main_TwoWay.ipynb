{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# from ipynb.fs.full.module import * # !pip install ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type='text/css'>\n",
       ".CodeMirror{\n",
       "font-size: 17px;\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%html\n",
    "# <style type='text/css'>\n",
    "# .CodeMirror{\n",
    "# font-size: 17px;\n",
    "# </style>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class params():\n",
    "    def __init__(self):\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder_act_func = 'tanh'\n",
    "        self.encoder_N_layers: int = 2    # number of RNN layers at encoder\n",
    "        self.encoder_N_neurons: int = 50  # number of neurons at each RNN\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder_N_layers: int = 2    # number of RNN layers at decoder\n",
    "        self.decoder_N_neurons: int = 50  # number of neurons at each RNN\n",
    "        self.decoder_bidirection = False   # True: bi-directional decoding, False: uni-directional decoding\n",
    "        self.attention_type: int = 4      # choose the attention type among five options\n",
    "        # 1. Only the last timestep (N-th)\n",
    "        # 2. Merge the last outputs of forward/backward RNN\n",
    "        # 3. Sum over all timesteps\n",
    "        # 4. Attention mechanism with N weights (same weight for forward/backward)\n",
    "        # 5. Attention mechanism with 2N weights (separate weights for forward/backward)\n",
    "        \n",
    "        # Setup\n",
    "        self.N_bits: int = 6                # number of bits\n",
    "        self.N_channel_use = 18             # number of channel uses\n",
    "        self.input_type = 'bit_vector'      # choose 'bit_vector' or 'one_hot_vector'\n",
    "        self.output_type = 'one_hot_vector' # choose 'bit_vector' or 'one_hot_vector'\n",
    "        self.decoder_info = 'None'          # 'bit_estimate', 'state_vector', 'None' for encoder input\n",
    "        self.encoder_info = 'tran_symbol'   # 'tran_symbol', 'state_vector', 'None' for decoder input\n",
    "\n",
    "        # Learning parameters\n",
    "        self.batch_size = int(2.5e4) \n",
    "#         self.batch_size = int(2e4) \n",
    "#         self.batch_size = int(1e4) \n",
    "        self.learning_rate = 0.01 \n",
    "        self.use_cuda = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Depending on decoder_info, the architecture is restricted\n",
    "# 1. If self.decoder_info = 'None', no restriction\n",
    "# 2. If self.decoder_info = 'bit_estimate', consider only uni-directional and immediate decoding\n",
    "# 3. If self.decoder_info = 'state_vector', consider only uni-directional (No need for immediate decoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model setup\n",
    "parameter = params()\n",
    "use_cuda = parameter.use_cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "np1:  0.7943282347242815\n",
      "np2:  0.01\n"
     ]
    }
   ],
   "source": [
    "# Generate training data\n",
    "SNR1 = 1               # SNR at User1 in dB\n",
    "np1 = 10**(-SNR1/10)   # noise power1 -- Assuming signal power is set to 1\n",
    "sigma1 = np.sqrt(np1)\n",
    "SNR2 = 20               # SNR at User2 in dB\n",
    "np2 = 10**(-SNR2/10)\n",
    "sigma2 = np.sqrt(np2)\n",
    "\n",
    "# Training set: tuples of (stream, noise1, noise 2)\n",
    "N_train = int(1e7)  # number of training set\n",
    "# N_train = int(1e5)\n",
    "bit1_train     = torch.randint(0, 2, (N_train, parameter.N_bits, 1))\n",
    "bit2_train     = torch.randint(0, 2, (N_train, parameter.N_bits, 1))\n",
    "noise1_train   = sigma1*torch.randn((N_train, parameter.N_channel_use, 1)) \n",
    "noise2_train   = sigma2*torch.randn((N_train, parameter.N_channel_use, 1)) \n",
    "\n",
    "# Validation\n",
    "N_validation = int(1e5)\n",
    "\n",
    "print('np1: ', np1)\n",
    "print('np2: ', np2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Twoway_coding(torch.nn.Module):\n",
    "    def __init__(self, param):\n",
    "        super(Twoway_coding, self).__init__()\n",
    "        \n",
    "        # import parameter\n",
    "        self.param        = param\n",
    "        if self.param.decoder_bidirection == True:\n",
    "            self.decoder_bi = 2 # bi-direction\n",
    "        else:\n",
    "            self.decoder_bi = 1 # uni-direction\n",
    "\n",
    "        ### Encoder input type\n",
    "        # 1. input_type (bit vector, one-hot vector) for Encoder\n",
    "        if self.param.input_type == 'bit_vector':\n",
    "            self.num_input = self.param.N_bits\n",
    "        elif self.param.input_type == 'one_hot_vector':\n",
    "            self.num_input = 2**self.param.N_bits\n",
    "        \n",
    "        # 2. Decoder Info (bit estimate, state vector) for Encoder\n",
    "        if self.param.decoder_info == 'bit_estimate':\n",
    "            self.num_D = self.param.N_bits\n",
    "        elif self.param.decoder_info == 'state_vector':\n",
    "            self.num_D = self.decoder_bi * self.param.decoder_N_neurons # 2*50 = 100\n",
    "        elif self.param.decoder_info == 'None':\n",
    "            self.num_D = 0\n",
    "        \n",
    "        ### Decoder input type\n",
    "        # 1. output_type (bits, one-hot vector) for Decoder\n",
    "        if self.param.output_type == 'bit_vector':\n",
    "            self.num_output = self.param.N_bits\n",
    "        elif self.param.output_type == 'one_hot_vector':\n",
    "            self.num_output = 2**self.param.N_bits\n",
    "            \n",
    "        # 2. Encoder Info ('tran_symbol', 'state_vector', 'None') for Decoder\n",
    "        if self.param.encoder_info == 'tran_symbol':\n",
    "            self.num_E = 1\n",
    "        elif self.param.encoder_info == 'state_vector':\n",
    "            self.num_E = self.param.encoder_N_neurons # 50\n",
    "        elif self.param.encoder_info == 'None':\n",
    "            self.num_E = 0\n",
    "            \n",
    "\n",
    "        # encoder 1. RNN\n",
    "        self.encoder1_RNN   = torch.nn.GRU(self.num_input + 1 + self.num_D, self.param.encoder_N_neurons, num_layers = self.param.encoder_N_layers, \n",
    "                                          bias=True, batch_first=True, dropout=0, bidirectional=False)\n",
    "        self.encoder1_linear = torch.nn.Linear(self.param.encoder_N_neurons, 1)\n",
    "        \n",
    "        # encoder 2. RNN\n",
    "        self.encoder2_RNN   = torch.nn.GRU(self.num_input + 1 + self.num_D, self.param.encoder_N_neurons, num_layers = self.param.encoder_N_layers, \n",
    "                                          bias=True, batch_first=True, dropout=0, bidirectional=False)\n",
    "        self.encoder2_linear = torch.nn.Linear(self.param.encoder_N_neurons, 1)\n",
    "\n",
    "        # power weight 1\n",
    "        self.weight_power1 = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use), requires_grad = True )\n",
    "        self.weight_power1.data.uniform_(1.0, 1.0) # all 1\n",
    "        self.weight_power1_normalized = torch.sqrt(self.weight_power1**2 *(self.param.N_channel_use)/torch.sum(self.weight_power1**2))\n",
    "        \n",
    "        # power weight 2\n",
    "        self.weight_power2 = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use), requires_grad = True )\n",
    "        self.weight_power2.data.uniform_(1.0, 1.0) # all 1\n",
    "        self.weight_power2_normalized = torch.sqrt(self.weight_power2**2 *(self.param.N_channel_use)/torch.sum(self.weight_power2**2))\n",
    "        \n",
    "        # decoder 1\n",
    "        self.decoder1_RNN = torch.nn.GRU(self.num_input + 1 + self.num_E, self.param.decoder_N_neurons, num_layers = self.param.decoder_N_layers, \n",
    "                                        bias=True, batch_first=True, dropout=0, bidirectional= self.param.decoder_bidirection) \n",
    "        self.decoder1_linear = torch.nn.Linear(self.decoder_bi*self.param.decoder_N_neurons, self.num_output) # 100,10\n",
    "        \n",
    "        # decoder 2\n",
    "        self.decoder2_RNN = torch.nn.GRU(self.num_input + 1 + self.num_E, self.param.decoder_N_neurons, num_layers = self.param.decoder_N_layers, \n",
    "                                        bias=True, batch_first=True, dropout=0, bidirectional= self.param.decoder_bidirection) \n",
    "        self.decoder2_linear = torch.nn.Linear(self.decoder_bi*self.param.decoder_N_neurons, self.num_output) # 100,10\n",
    "\n",
    "        \n",
    "        # attention type\n",
    "        if self.param.attention_type==5:  # bi-directional --> 2N weights\n",
    "            self.weight1_merge = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use,2), requires_grad = True ) \n",
    "            self.weight1_merge.data.uniform_(1.0, 1.0) # all 1\n",
    "            # Normalization\n",
    "            self.weight1_merge_normalized_fwd = torch.sqrt(self.weight1_merge[:,0]**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge[:,0]**2)) \n",
    "            self.weight1_merge_normalized_bwd  = torch.sqrt(self.weight1_merge[:,1]**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge[:,1]**2))\n",
    "        \n",
    "            self.weight2_merge = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use,2), requires_grad = True ) \n",
    "            self.weight2_merge.data.uniform_(1.0, 1.0) # all 1\n",
    "            # Normalization\n",
    "            self.weight2_merge_normalized_fwd = torch.sqrt(self.weight2_merge[:,0]**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge[:,0]**2)) \n",
    "            self.weight2_merge_normalized_bwd  = torch.sqrt(self.weight2_merge[:,1]**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge[:,1]**2))\n",
    "        \n",
    "        if self.param.attention_type== 4: # uni-directional --> N weights\n",
    "            self.weight1_merge = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use),requires_grad = True )\n",
    "            self.weight1_merge.data.uniform_(1.0, 1.0) # all 1\n",
    "            # Normalization\n",
    "            self.weight1_merge_normalized  = torch.sqrt(self.weight1_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge**2))\n",
    "        \n",
    "            self.weight2_merge = torch.nn.Parameter(torch.Tensor(self.param.N_channel_use),requires_grad = True )\n",
    "            self.weight2_merge.data.uniform_(1.0, 1.0) # all 1\n",
    "            # Normalization\n",
    "            self.weight2_merge_normalized  = torch.sqrt(self.weight2_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge**2))\n",
    "        \n",
    "        \n",
    "        # Parameters for normalization (mean and variance)\n",
    "        # User 1\n",
    "        self.mean1_batch = torch.zeros(self.param.N_channel_use) \n",
    "        self.std1_batch = torch.ones(self.param.N_channel_use)\n",
    "        self.mean1_saved = torch.zeros(self.param.N_channel_use)\n",
    "        self.std1_saved = torch.ones(self.param.N_channel_use)\n",
    "        # User 2\n",
    "        self.mean2_batch = torch.zeros(self.param.N_channel_use) \n",
    "        self.std2_batch = torch.ones(self.param.N_channel_use)\n",
    "        self.mean2_saved = torch.zeros(self.param.N_channel_use)\n",
    "        self.std2_saved = torch.ones(self.param.N_channel_use)\n",
    "        self.normalization_with_saved_data = False   # True: inference with saved mean/var, False: calculate mean/var\n",
    "\n",
    "    def decoder_activation(self, inputs):\n",
    "        if self.param.output_type == 'bit_vector':\n",
    "            return torch.sigmoid(inputs) # training with binary cross entropy\n",
    "        elif self.param.output_type == 'one_hot_vector':\n",
    "            return inputs # Note. softmax function is applied in \"F.cross_entropy\" function\n",
    "    \n",
    "    # Convert `bit vector' to 'one-hot vector'\n",
    "    def one_hot(self, bit_vec):\n",
    "        bit_vec = bit_vec.view(parameter.batch_size, parameter.N_bits)\n",
    "        N_batch = bit_vec.size(0) # batch_size\n",
    "        N_bits = bit_vec.size(1)  # N_bits=K\n",
    "\n",
    "        ind = torch.arange(0,N_bits).repeat(N_batch,1) \n",
    "        ind = ind.to(device)\n",
    "        ind_vec = torch.sum( torch.mul(bit_vec, 2**ind), axis=1 ).long()\n",
    "        bit_onehot = torch.zeros((N_batch, 2**N_bits), dtype=int)\n",
    "        for ii in range(N_batch):\n",
    "            bit_onehot[ii, ind_vec[ii]]=1 # one-hot vector\n",
    "        return bit_onehot \n",
    "        \n",
    "    def normalization(self, inputs, t_idx, user_idx):\n",
    "        if self.training: # During training\n",
    "            mean_batch = torch.mean(inputs)\n",
    "            std_batch  = torch.std(inputs)\n",
    "            outputs   = (inputs - mean_batch)/std_batch\n",
    "        else: \n",
    "            if self.normalization_with_saved_data: # During inference\n",
    "                if user_idx==1:\n",
    "                    outputs   = (inputs - self.mean1_saved[t_idx])/self.std1_saved[t_idx]\n",
    "                elif user_idx==2:\n",
    "                    outputs   = (inputs - self.mean2_saved[t_idx])/self.std2_saved[t_idx]\n",
    "            else: \n",
    "                # During validation\n",
    "                mean_batch = torch.mean(inputs)\n",
    "                std_batch  = torch.std(inputs)\n",
    "                outputs   = (inputs - mean_batch)/std_batch\n",
    "                # calculate mean/var after training\n",
    "                if user_idx==1:\n",
    "                    self.mean1_batch[t_idx] = mean_batch\n",
    "                    self.std1_batch[t_idx] = std_batch\n",
    "                elif user_idx==2:\n",
    "                    self.mean2_batch[t_idx] = mean_batch\n",
    "                    self.std2_batch[t_idx] = std_batch\n",
    "        return outputs\n",
    "\n",
    "\n",
    "    def forward(self, b1, b2, noise1, noise2):\n",
    "\n",
    "        # Normalize power weights\n",
    "        self.weight_power1_normalized  = torch.sqrt(self.weight_power1**2 *(self.param.N_channel_use)/torch.sum(self.weight_power1**2))\n",
    "        self.weight_power2_normalized  = torch.sqrt(self.weight_power2**2 *(self.param.N_channel_use)/torch.sum(self.weight_power2**2))\n",
    "        \n",
    "        # Encoder input\n",
    "        if self.param.input_type == 'bit_vector':\n",
    "            I1 = b1 \n",
    "            I2 = b2 \n",
    "        elif self.param.input_type == 'one_hot_vector':\n",
    "            I1 = self.one_hot(b1).to(device)\n",
    "            I2 = self.one_hot(b2).to(device)\n",
    "        \n",
    "        for t in range(self.param.N_channel_use): # timesteps\n",
    "            # Encoder\n",
    "            if t == 0: # 1st timestep\n",
    "                input1_total        = torch.cat([I1.view(self.param.batch_size, 1, self.num_input), \n",
    "                                               torch.zeros((self.param.batch_size, 1, self.num_D+1)).to(device)], dim=2) \n",
    "                ### input1_total   -- (batch, 1, num_input + num_D + 1) \n",
    "                x1_t_after_RNN, s1_t_hidden  = self.encoder1_RNN(input1_total)\n",
    "                ### x1_t_after_RNN -- (batch, 1, hidden)\n",
    "                ### s1_t_hidden    -- (layers, batch, hidden)\n",
    "                x1_t_tilde =   self.encoder1_linear(x1_t_after_RNN)\n",
    "                import sys\n",
    "                print(x1_t_tilde);sys.exit()\n",
    "                x1_t_tilde =   torch.tanh(self.encoder1_linear(x1_t_after_RNN))   \n",
    "                \n",
    "                input2_total        = torch.cat([I2.view(self.param.batch_size, 1, self.num_input), \n",
    "                                               torch.zeros((self.param.batch_size, 1, self.num_D+1)).to(device)], dim=2) \n",
    "                x2_t_after_RNN, s2_t_hidden  = self.encoder2_RNN(input2_total)\n",
    "                x2_t_tilde =   torch.tanh(self.encoder2_linear(x2_t_after_RNN))   \n",
    "                \n",
    "                \n",
    "            else: # 2nd-Nth timestep\n",
    "                if self.param.decoder_info == 'None':\n",
    "                    input1_total        = torch.cat([I1.view(self.param.batch_size, 1, self.num_input), y1_t], dim=2) \n",
    "                    input2_total        = torch.cat([I2.view(self.param.batch_size, 1, self.num_input), y2_t], dim=2) \n",
    "                else:\n",
    "                    input1_total        = torch.cat([I1.view(self.param.batch_size, 1, self.num_input), y1_t, D1_tmp], dim=2) \n",
    "                    input2_total        = torch.cat([I2.view(self.param.batch_size, 1, self.num_input), y2_t, D2_tmp], dim=2)\n",
    "                \n",
    "                x1_t_after_RNN, s1_t_hidden  = self.encoder1_RNN(input1_total, s1_t_hidden)\n",
    "                x1_t_tilde =   torch.tanh(self.encoder1_linear(x1_t_after_RNN))\n",
    "                \n",
    "                \n",
    "                x2_t_after_RNN, s2_t_hidden  = self.encoder2_RNN(input2_total, s2_t_hidden)\n",
    "                x2_t_tilde =   torch.tanh(self.encoder2_linear(x2_t_after_RNN))\n",
    "            \n",
    "            # Power control layer: 1. Normalization, 2. Power allocation\n",
    "            x1_t_norm = self.normalization(x1_t_tilde, t, 1).view(self.param.batch_size, 1, 1)\n",
    "            x1_t  = x1_t_norm * self.weight_power1_normalized[t] \n",
    "            x2_t_norm = self.normalization(x2_t_tilde, t, 2).view(self.param.batch_size, 1, 1)\n",
    "            x2_t  = x2_t_norm * self.weight_power2_normalized[t] \n",
    "            \n",
    "            # Forward transmission (from User 1 to 2)\n",
    "            y2_t = x1_t + noise1[:,t,:].view(self.param.batch_size, 1, 1)\n",
    "            \n",
    "            # Backward transmission (from User 2 to 1)\n",
    "            y1_t = x2_t + noise2[:,t,:].view(self.param.batch_size, 1, 1)\n",
    "            \n",
    "            # Concatenate values along time t\n",
    "            if t == 0:\n",
    "#                 x1_norm_total = x1_t_norm\n",
    "                x1_total = x1_t\n",
    "                x2_total = x2_t\n",
    "                y1_total = y1_t\n",
    "                y2_total = y2_t\n",
    "            else:\n",
    "#                 x_norm_total = torch.cat([x_norm_total, x_t_norm], dim=1) \n",
    "                x1_total = torch.cat([x1_total, x1_t ], dim = 1) # In the end, (batch, N, 1)\n",
    "                x2_total = torch.cat([x2_total, x2_t ], dim = 1)\n",
    "                y1_total = torch.cat([y1_total, y1_t ], dim = 1) \n",
    "                y2_total = torch.cat([y2_total, y2_t ], dim = 1) \n",
    "            \n",
    "            # Encoder info updates\n",
    "            if self.param.encoder_info == 'tran_symbol':\n",
    "                E1_tmp = x1_t # (batch,1,1)\n",
    "                E2_tmp = x2_t # (batch,1,1)\n",
    "            elif self.param.encoder_info == 'state_vector':\n",
    "                E1_tmp = x1_t_after_RNN # (batch, 1, hidden) \n",
    "                E2_tmp = x2_t_after_RNN # (batch, 1, hidden) \n",
    "#                 E1_tmp = s1_t_hidden[-1].view(self.param.batch_size, 1, -1) # (batch, 1, hidden) # only last layer\n",
    "#                 E2_tmp = s2_t_hidden[-1].view(self.param.batch_size, 1, -1) # (batch, 1, hidden)\n",
    "            elif self.param.encoder_info == 'None':\n",
    "                E1_tmp = None\n",
    "                E2_tmp = None\n",
    "            \n",
    "            ########################################\n",
    "            # Immediate Decoding\n",
    "            if self.param.decoder_info != 'None': \n",
    "                # Encoder uses decoder info\n",
    "                # --> There is connection from decoder to encoder\n",
    "                # --> Immediate decoding is needed!\n",
    "                if self.param.encoder_info == 'None': # Decoder does not use encoder info\n",
    "                    decoder_input1 = torch.cat([I1.view(self.param.batch_size, 1, self.num_input), y1_t], dim=2) # (batch, 1, num_input + 1)\n",
    "                    decoder_input2 = torch.cat([I2.view(self.param.batch_size, 1, self.num_input), y2_t], dim=2) # (batch, 1, num_input + 1)\n",
    "                else:\n",
    "                    decoder_input1 = torch.cat([I1.view(self.param.batch_size, 1, self.num_input), y1_t, E1_tmp], dim=2) # (batch, 1, num_input + 1 + num_E)\n",
    "                    decoder_input2 = torch.cat([I2.view(self.param.batch_size, 1, self.num_input), y2_t, E2_tmp], dim=2)\n",
    "    \n",
    "                ### decoder_input1: (batch, 1, num_input + 1 + num_E) -- batch, input seq, input size\n",
    "                r1_t_last, r1_t_hidden  = self.decoder1_RNN(decoder_input1)\n",
    "                ### r1_t_last   -- (batch, 1(=input seq), hidden)\n",
    "                ### r1_t_hidden -- (layer, batch, hidden)\n",
    "                r2_t_last, r2_t_hidden  = self.decoder2_RNN(decoder_input2)\n",
    "                \n",
    "                if self.param.decoder_info == 'state_vector': # No output calculation required\n",
    "                    D1_tmp = r1_t_last\n",
    "                    D2_tmp = r2_t_last\n",
    "                    if t==0:\n",
    "                        r1_hidden = r1_t_last\n",
    "                        r2_hidden = r2_t_last\n",
    "                    else:\n",
    "                        r1_hidden = torch.cat([r1_hidden, r1_t_last], dim=1) # Finally, (batch, N, hidden)\n",
    "                        r2_hidden = torch.cat([r2_hidden, r2_t_last], dim=1)\n",
    "                        \n",
    "#                 if self.decoder_info == 'bit_estimate':\n",
    "                \n",
    "#                     output1     = self.decoder_activation(self.decoder1_linear(z1_t_after_RNN)) # (batch,1,num_output)\n",
    "#                     output1_last = output1.view(self.param.batch_size,-1,1) # (batch, num_output, 1)\n",
    "                \n",
    "        # Decoder do inference after N transmission are conducted!   \n",
    "        if self.param.decoder_info == 'state_vector':\n",
    "            # Only Uni-directinoal is possible\n",
    "            \n",
    "            # Normalize attention weights (Uni-directional attention weights)\n",
    "            self.weight1_merge_normalized  = torch.sqrt(self.weight1_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge**2)) \n",
    "            self.weight2_merge_normalized  = torch.sqrt(self.weight2_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge**2)) \n",
    "            \n",
    "            # Multiply attention weights\n",
    "            r1_merge = torch.tensordot(r1_hidden, self.weight1_merge_normalized, dims=([1], [0])) # (batch, hidden)\n",
    "            output1 = self.decoder_activation(self.decoder1_linear(r1_merge)) \n",
    "            output1_last = output1.view(self.param.batch_size,-1,1) # (batch, num_output, 1)\n",
    "\n",
    "            r2_merge = torch.tensordot(r2_hidden, self.weight2_merge_normalized, dims=([1], [0])) # (batch, hidden)\n",
    "            output2 = self.decoder_activation(self.decoder2_linear(r2_merge)) \n",
    "            output2_last = output2.view(self.param.batch_size,-1,1) # (batch, num_output, 1)\n",
    "    \n",
    "    \n",
    "        # Non-immediate Decoding\n",
    "        if self.param.decoder_info == 'None': # No connection from decoder to encoder\n",
    "            # Normalize attention weights\n",
    "            if parameter.attention_type== 4:\n",
    "                self.weight1_merge_normalized  = torch.sqrt(self.weight1_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge**2)) \n",
    "                self.weight2_merge_normalized  = torch.sqrt(self.weight2_merge**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge**2)) \n",
    "            if parameter.attention_type== 5:\n",
    "                self.weight1_merge_normalized_fwd  = torch.sqrt(self.weight1_merge[:,0]**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge[:,0]**2)) # 30\n",
    "                self.weight1_merge_normalized_bwd  = torch.sqrt(self.weight1_merge[:,1]**2 *(self.param.N_channel_use)/torch.sum(self.weight1_merge[:,1]**2))\n",
    "                self.weight2_merge_normalized_fwd  = torch.sqrt(self.weight2_merge[:,0]**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge[:,0]**2)) # 30\n",
    "                self.weight2_merge_normalized_bwd  = torch.sqrt(self.weight2_merge[:,1]**2 *(self.param.N_channel_use)/torch.sum(self.weight2_merge[:,1]**2))\n",
    "\n",
    "                \n",
    "            I1_tmp = I1.view(self.param.batch_size, 1, self.num_input)\n",
    "            I1_copy = I1_tmp.repeat(1, self.param.N_channel_use, 1) # (batch, N, K)\n",
    "            I2_tmp = I2.view(self.param.batch_size, 1, self.num_input)\n",
    "            I2_copy = I2_tmp.repeat(1, self.param.N_channel_use, 1) # (batch, N, K)\n",
    "            if self.param.encoder_info == 'None':\n",
    "                decoder1_input = torch.cat([I1_copy, y1_total], dim=2) # (batch, N, K+1)\n",
    "                decoder2_input = torch.cat([I2_copy, y2_total], dim=2) # (batch, N, K+1)\n",
    "            elif self.param.encoder_info == 'tran_symbol':\n",
    "                decoder1_input = torch.cat([I1_copy, x1_total, y1_total], dim=2) # (batch, N, K+2)\n",
    "                decoder2_input = torch.cat([I2_copy, x2_total, y2_total], dim=2) # (batch, N, K+2)\n",
    "            \n",
    "            r1_hidden, _  = self.decoder1_RNN(decoder1_input) # (batch, N, bi*hidden_size)\n",
    "            r2_hidden, _  = self.decoder2_RNN(decoder2_input) # (batch, N, bi*hidden_size)\n",
    "\n",
    "\n",
    "    #         # Option 1. Only the N-th timestep\n",
    "    #         if parameter.attention_type== 1:\n",
    "    #             output     = self.decoder_activation(self.decoder_linear(r_hidden)) #(batch,N,bi*hidden)-->(batch,N,num_output)\n",
    "    #             output_last = output[:,-1,:].view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "    #         # Option 2. Merge the \"last\" outputs of forward/backward RNN\n",
    "    #         if parameter.attention_type== 2:\n",
    "    #             r_backward = r_hidden[:,0,self.param.decoder_N_neurons:] # Output at the 1st timestep of reverse RNN \n",
    "    #             r_forward = r_hidden[:,-1,:self.param.decoder_N_neurons] # Output at the N-th timestep of forward RNN\n",
    "    #             r_concat = torch.cat([r_backward, r_forward ], dim = 1) \n",
    "    #             output = self.decoder_activation(self.decoder_linear(r_concat)) # (batch,num_output)\n",
    "    #             output_last = output.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "    #         # Option 3. Sum over all timesteps\n",
    "    #         if parameter.attention_type== 3:\n",
    "    #             output     = self.decoder_activation(self.decoder_linear(r_hidden)) \n",
    "    #             output_last = torch.sum(output, dim=1).view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "            # Option 4. Attention mechanism (N weights)\n",
    "            if parameter.attention_type== 4:\n",
    "                r1_concat = torch.tensordot(r1_hidden, self.weight1_merge_normalized, dims=([1], [0])) # (batch, hidden_size)\n",
    "                output1 = self.decoder_activation(self.decoder1_linear(r1_concat)) \n",
    "                output1_last = output1.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "                \n",
    "                r2_concat = torch.tensordot(r2_hidden, self.weight2_merge_normalized, dims=([1], [0])) # (batch, hidden_size)\n",
    "                output2 = self.decoder_activation(self.decoder2_linear(r2_concat)) \n",
    "                output2_last = output2.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "            # Option 5. Attention mechanism (2N weights) for forward/backward\n",
    "            if parameter.attention_type== 5:\n",
    "                r1_hidden_forward = r1_hidden[:,:,:self.param.decoder_N_neurons]  # (batch,num_output,hidden_size)\n",
    "                r1_hidden_backward = r1_hidden[:,:,self.param.decoder_N_neurons:] # (batch,num_output,hidden_size)\n",
    "                r1_forward_weighted_sum = torch.tensordot(r1_hidden_forward, self.weight1_merge_normalized_fwd, dims=([1], [0]))  # (batch,hidden_size)\n",
    "                r1_backward_weighted_sum = torch.tensordot(r1_hidden_backward, self.weight1_merge_normalized_bwd, dims=([1], [0]))         # (batch,hidden_size)\n",
    "                r1_concat = torch.cat([r1_forward_weighted_sum, r1_backward_weighted_sum], dim = 1) \n",
    "                output1 = self.decoder_activation(self.decoder1_linear(r1_concat)) \n",
    "                output1_last = output1.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "                r2_hidden_forward = r2_hidden[:,:,:self.param.decoder_N_neurons]  # (batch,num_output,hidden_size)\n",
    "                r2_hidden_backward = r2_hidden[:,:,self.param.decoder_N_neurons:] # (batch,num_output,hidden_size)\n",
    "                r2_forward_weighted_sum = torch.tensordot(r2_hidden_forward, self.weight2_merge_normalized_fwd, dims=([1], [0]))  # (batch,hidden_size)\n",
    "                r2_backward_weighted_sum = torch.tensordot(r2_hidden_backward, self.weight2_merge_normalized_bwd, dims=([1], [0]))         # (batch,hidden_size)\n",
    "                r2_concat = torch.cat([r2_forward_weighted_sum, r2_backward_weighted_sum], dim = 1) \n",
    "                output2 = self.decoder_activation(self.decoder2_linear(r2_concat)) \n",
    "                output2_last = output2.view(self.param.batch_size,-1,1) # (batch,num_output,1)\n",
    "\n",
    "            \n",
    "        self.x1 = x1_total                    # (batch,N,1)\n",
    "        self.x2 = x2_total                    # (batch,N,1)\n",
    "        \n",
    "        return output1_last, output2_last"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the `bit vector' with (batch,K,1) to 'one hot vector' with (batch,2^K)\n",
    "def one_hot(bit_vec):\n",
    "    bit_vec = bit_vec.squeeze(-1)  # (batch, K)\n",
    "    N_batch = bit_vec.size(0) \n",
    "    N_bits = bit_vec.size(1)\n",
    "\n",
    "    ind = torch.arange(0,N_bits).repeat(N_batch,1) # (batch, K)\n",
    "    ind = ind.to(device)\n",
    "    ind_vec = torch.sum( torch.mul(bit_vec,2**ind), axis=1).long() # batch\n",
    "    b_onehot = torch.zeros((N_batch, 2**N_bits), dtype=int)\n",
    "    for ii in range(N_batch):\n",
    "        b_onehot[ii, ind_vec[ii]]=1 # one-hot vector\n",
    "    return b_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test\n",
    "def test_RNN(N_test): \n",
    "\n",
    "    # Generate test data\n",
    "    bit1_test     = torch.randint(0, 2, (N_test, parameter.N_bits, 1)) \n",
    "    bit2_test     = torch.randint(0, 2, (N_test, parameter.N_bits, 1)) \n",
    "    noise1_test  = sigma1*torch.randn((N_test, parameter.N_channel_use,1))\n",
    "    noise2_test   = sigma2*torch.randn((N_test, parameter.N_channel_use,1))\n",
    "    \n",
    "    model.eval() # model.training() becomes False\n",
    "    N_iter = (N_test//parameter.batch_size) # N_test should be multiple of batch_size\n",
    "    ber1=0\n",
    "    bler1=0\n",
    "    ber2=0\n",
    "    bler2=0\n",
    "    power1_acc = np.zeros((parameter.batch_size, parameter.N_channel_use ,1))\n",
    "    power2_acc = np.zeros((parameter.batch_size, parameter.N_channel_use ,1))\n",
    "    with torch.no_grad():\n",
    "        for i in range(N_iter):\n",
    "            bit1 = bit1_test[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_bits,1) # batch, K,1\n",
    "            bit2 = bit2_test[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_bits,1)\n",
    "            noise1 = noise1_test[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1) # batch, N,1\n",
    "            noise2 = noise2_test[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1) # batch, N,1\n",
    "\n",
    "            bit1 = bit1.to(device)\n",
    "            bit2 = bit2.to(device)\n",
    "            noise1 = noise1.to(device)\n",
    "            noise2 = noise2.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            X2_hat, X1_hat = model(bit1, bit2, noise1, noise2)\n",
    "\n",
    "            if parameter.output_type == 'bit_vector':\n",
    "                ber1_tmp, bler1_tmp = error_rate_bitvector(X1_hat.cpu(), bit1.cpu())\n",
    "                ber2_tmp, bler2_tmp = error_rate_bitvector(X2_hat.cpu(), bit2.cpu())\n",
    "            elif parameter.output_type == 'one_hot_vector':\n",
    "                ber1_tmp, bler1_tmp = error_rate_onehot(X1_hat.cpu(), bit1.cpu())\n",
    "                ber2_tmp, bler2_tmp = error_rate_onehot(X2_hat.cpu(), bit2.cpu())\n",
    "                \n",
    "            ber1 = ber1 + ber1_tmp\n",
    "            ber2 = ber2 + ber2_tmp\n",
    "            bler1 = bler1 + bler1_tmp\n",
    "            bler2 = bler2 + bler2_tmp\n",
    "            \n",
    "            # Power\n",
    "            signal1 = model.x1.cpu().detach().numpy()\n",
    "            power1_acc += signal1**2 \n",
    "            signal2 = model.x2.cpu().detach().numpy()\n",
    "            power2_acc += signal2**2 \n",
    "            \n",
    "        ber1  = ber1/N_iter\n",
    "        ber2  = ber2/N_iter\n",
    "        bler1 = bler1/N_iter\n",
    "        bler2 = bler2/N_iter\n",
    "        power1_avg = power1_acc/N_iter\n",
    "        power2_avg = power2_acc/N_iter\n",
    "\n",
    "    return ber1, ber2, bler1, bler2, power1_avg, power2_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twoway_coding(\n",
      "  (encoder1_RNN): GRU(7, 50, num_layers=2, batch_first=True)\n",
      "  (encoder1_linear): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (encoder2_RNN): GRU(7, 50, num_layers=2, batch_first=True)\n",
      "  (encoder2_linear): Linear(in_features=50, out_features=1, bias=True)\n",
      "  (decoder1_RNN): GRU(8, 50, num_layers=2, batch_first=True)\n",
      "  (decoder1_linear): Linear(in_features=50, out_features=64, bias=True)\n",
      "  (decoder2_RNN): GRU(8, 50, num_layers=2, batch_first=True)\n",
      "  (decoder2_linear): Linear(in_features=50, out_features=64, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if use_cuda:\n",
    "    model = Twoway_coding(parameter).to(device)\n",
    "else:\n",
    "    model = Twoway_coding(parameter)\n",
    "\n",
    "print(model)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=parameter.learning_rate)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training \n",
      "weight_power1:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "weight_power2:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "weight1_merge:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "weight2_merge:  [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 1.09 GiB (GPU 1; 15.90 GiB total capacity; 4.41 GiB already allocated; 657.75 MiB free; 4.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-03591219d331>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m         \u001b[0mX2_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX1_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbit1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbit2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoise2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# Define loss according to output type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-3022e4a13bba>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, b1, b2, noise1, noise2)\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mr1_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder1_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder1_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch, N, bi*hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mr2_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder2_RNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdecoder2_input\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# (batch, N, bi*hidden_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-torch/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/my-torch/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    848\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             result = _VF.gru(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 850\u001b[0;31m                              self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    851\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m             result = _VF.gru(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 1.09 GiB (GPU 1; 15.90 GiB total capacity; 4.41 GiB already allocated; 657.75 MiB free; 4.61 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "num_epoch = 100\n",
    "clipping_value = 1\n",
    " \n",
    "print('Before training ')\n",
    "print('weight_power1: ', model.weight_power1_normalized.cpu().detach().numpy().round(3))\n",
    "print('weight_power2: ', model.weight_power2_normalized.cpu().detach().numpy().round(3))\n",
    "if parameter.attention_type==4:\n",
    "    print('weight1_merge: ', model.weight1_merge_normalized.cpu().detach().numpy().round(3))\n",
    "    print('weight2_merge: ', model.weight2_merge_normalized.cpu().detach().numpy().round(3))\n",
    "if parameter.attention_type==5:\n",
    "    print('weight1_merge_fwd: ', model.weight1_merge_normalized_fwd.cpu().detach().numpy().round(3))\n",
    "    print('weight1_merge_bwd: ', model.weight1_merge_normalized_bwd.cpu().detach().numpy().round(3))\n",
    "    print('weight2_merge_fwd: ', model.weight2_merge_normalized_fwd.cpu().detach().numpy().round(3))\n",
    "    print('weight2_merge_bwd: ', model.weight2_merge_normalized_bwd.cpu().detach().numpy().round(3))\n",
    "print()\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "\n",
    "    model.train() # model.training() becomes True\n",
    "    loss_training = 0\n",
    "    \n",
    "    N_iter = (N_train//parameter.batch_size)\n",
    "    for i in range(N_iter):\n",
    "        bit1 = bit1_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_bits,1) \n",
    "        bit2 = bit2_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_bits,1) \n",
    "        noise1 = noise1_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1)\n",
    "        noise2 = noise2_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1)\n",
    "\n",
    "        bit1   = bit1.to(device)\n",
    "        bit2   = bit2.to(device)\n",
    "        noise1 = noise1.to(device)\n",
    "        noise2 = noise2.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        optimizer.zero_grad() \n",
    "        X2_hat, X1_hat = model(bit1, bit2, noise1, noise2)\n",
    "\n",
    "        # Define loss according to output type\n",
    "        if parameter.output_type == 'bit_vector':\n",
    "            loss = F.binary_cross_entropy(X1_hat, bit1) + F.binary_cross_entropy(X2_hat, bit2)\n",
    "        elif parameter.output_type == 'one_hot_vector':\n",
    "            bit1_hot =  one_hot(bit1).view(parameter.batch_size, 2**parameter.N_bits, 1) # (batch,2^K,1)\n",
    "            bit2_hot =  one_hot(bit2).view(parameter.batch_size, 2**parameter.N_bits, 1)\n",
    "            loss = (F.cross_entropy(X1_hat.squeeze(-1), torch.argmax(bit1_hot,dim=1).squeeze(-1).to(device)) # (batch,2^K), (batch)\n",
    "                    + F.cross_entropy(X2_hat.squeeze(-1), torch.argmax(bit2_hot,dim=1).squeeze(-1).to(device)))\n",
    "        \n",
    "        # training\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clipping_value)\n",
    "        loss_training += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print('Epoch: {}, Iter: {} out of {}, Loss: {:.4f}'.format(epoch, i, N_iter, loss.item()))\n",
    "\n",
    "    # Summary of each epoch\n",
    "    print('Summary: Epoch: {}, lr: {}, Average loss: {:.4f}'.format(epoch, optimizer.param_groups[0]['lr'], loss_training/N_iter) )\n",
    "\n",
    "    scheduler.step() # reduce learning rate\n",
    "    \n",
    "    print('weight_power1: ', model.weight_power1_normalized.cpu().detach().numpy().round(3))\n",
    "    print('weight_power2: ', model.weight_power2_normalized.cpu().detach().numpy().round(3))\n",
    "    if parameter.attention_type==4:\n",
    "        print('weight1_merge: ', model.weight1_merge_normalized.cpu().detach().numpy().round(3))\n",
    "        print('weight2_merge: ', model.weight2_merge_normalized.cpu().detach().numpy().round(3))\n",
    "    if parameter.attention_type==5:\n",
    "        print('weight1_merge_fwd: ', model.weight1_merge_normalized_fwd.cpu().detach().numpy().round(3))\n",
    "        print('weight1_merge_bwd: ', model.weight1_merge_normalized_bwd.cpu().detach().numpy().round(3))\n",
    "        print('weight2_merge_fwd: ', model.weight2_merge_normalized_fwd.cpu().detach().numpy().round(3))\n",
    "        print('weight2_merge_bwd: ', model.weight2_merge_normalized_bwd.cpu().detach().numpy().round(3))\n",
    "    print()\n",
    "    \n",
    "    # Validation\n",
    "    ber1_val, ber2_val, bler1_val, bler2_val, _, _ = test_RNN(N_validation)\n",
    "    print('Ber1:  ', float(ber1_val))\n",
    "    print('Ber2:  ', float(ber2_val))\n",
    "    print('Bler1: ', float(bler1_val))\n",
    "    print('Bler2: ', float(bler2_val))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calculate mean/var with training data\n",
    "model.eval()   # model.training() becomes False\n",
    "N_iter = N_train//parameter.batch_size\n",
    "mean1_train = torch.zeros(parameter.N_channel_use)\n",
    "std1_train  = torch.zeros(parameter.N_channel_use)\n",
    "mean2_train = torch.zeros(parameter.N_channel_use)\n",
    "std2_train  = torch.zeros(parameter.N_channel_use)\n",
    "mean1_total = 0\n",
    "std1_total = 0\n",
    "mean2_total = 0\n",
    "std2_total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(N_iter):\n",
    "        bit1   = bit1_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_bits,1) \n",
    "        bit2   = bit2_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_bits,1) \n",
    "        noise1 = noise1_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1)\n",
    "        noise2 = noise2_train[parameter.batch_size*i:parameter.batch_size*(i+1),:,:].view(parameter.batch_size, parameter.N_channel_use,1)\n",
    "\n",
    "        bit1   = bit1.to(device)\n",
    "        bit2   = bit2.to(device)\n",
    "        noise1 = noise1.to(device)\n",
    "        noise2 = noise2.to(device)\n",
    "        \n",
    "        X2_hat, X1_hat = model(bit1, bit2, noise1, noise2)\n",
    "        mean1_total += model.mean1_batch\n",
    "        std1_total  += model.std1_batch\n",
    "        mean2_total += model.mean2_batch\n",
    "        std2_total  += model.std2_batch\n",
    "        if i%100==0: print(i)\n",
    "        \n",
    "mean1_train = mean1_total/N_iter\n",
    "std1_train = std1_total/N_iter\n",
    "mean2_train = mean2_total/N_iter\n",
    "std2_train = std2_total/N_iter\n",
    "print('Mean1: ',mean1_train)\n",
    "print('std1 : ',std1_train)\n",
    "print('Mean2: ',mean2_train)\n",
    "print('std2 : ',std2_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Inference stage\n",
    "N_inference = int(4e8) \n",
    "# N_inference = int(4e9) \n",
    "# N_inference = int(1e10) \n",
    "N_small = int(1e5) # In case that N_inference is very large, we divide into small chunks\n",
    "N_iter  = N_inference//N_small\n",
    "\n",
    "model.normalization_with_saved_data = True\n",
    "model.mean1_saved = mean1_train\n",
    "model.std1_saved  = std1_train\n",
    "model.mean2_saved = mean2_train\n",
    "model.std2_saved  = std2_train\n",
    "\n",
    "ber1_sum  = 0\n",
    "bler1_sum = 0\n",
    "ber2_sum  = 0\n",
    "bler2_sum = 0\n",
    "power1_sum = np.zeros((parameter.batch_size, parameter.N_channel_use ,1))\n",
    "power2_sum = np.zeros((parameter.batch_size, parameter.N_channel_use ,1))\n",
    "\n",
    "for ii in range(N_iter):\n",
    "    ber1_tmp, ber2_tmp, bler1_tmp, bler2_tmp, power1_tmp, power2_tmp = test_RNN(N_small)\n",
    "    ber1_sum += ber1_tmp\n",
    "    ber2_sum += ber2_tmp\n",
    "    bler1_sum += bler1_tmp\n",
    "    bler2_sum += bler2_tmp\n",
    "    power1_sum += power1_tmp # (batch, N, 1)\n",
    "    power2_sum += power2_tmp\n",
    "    if ii%100==0: \n",
    "        print('Iter: {} out of {}'.format(ii, N_iter))\n",
    "        print('Ber1:  ', float(ber1_sum/(ii+1)))\n",
    "        print('Ber2:  ', float(ber2_sum/(ii+1)))\n",
    "        print('Bler1: ', float(bler1_sum/(ii+1)))\n",
    "        print('Bler2: ', float(bler2_sum/(ii+1)))\n",
    "        print('Power1: ', round(np.sum(power1_sum)/(parameter.batch_size*(ii+1)),3))\n",
    "        print('Power2: ', round(np.sum(power2_sum)/(parameter.batch_size*(ii+1)),3))\n",
    "\n",
    "ber1_inference  = ber1_sum/N_iter\n",
    "ber2_inference  = ber2_sum/N_iter\n",
    "bler1_inference = bler1_sum/N_iter\n",
    "bler2_inference = bler2_sum/N_iter\n",
    "\n",
    "print()\n",
    "print('Ber1:  ', float(ber1_inference))\n",
    "print('Ber2:  ', float(ber2_inference))\n",
    "print('Bler1: ', float(bler1_inference))\n",
    "print('Bler2: ', float(bler2_inference))\n",
    "print('Power1: ', round(np.sum(power1_sum)/(parameter.batch_size*N_iter),3))\n",
    "print('Power2: ', round(np.sum(power2_sum)/(parameter.batch_size*N_iter),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Save model\n",
    "save_results_to = 'saved_model/'+ 'K6_N18/uni/SNR1(-5dB)SNR2(09dB)/'\n",
    "# save_results_to = 'saved_model/'+ 'info/bi(5)_tx_null/'\n",
    "# save_results_to = 'saved_model/'+ 'K6_N18/np2(0)/'\n",
    "\n",
    "torch.save(model.state_dict(), save_results_to+'model.pth')\n",
    "# np2 = sigma2_train**2\n",
    "# save_results_to = 'saved_model/'+ 'np2_'+ str(np2)+'/'\n",
    "\n",
    "####### Save normalization weights\n",
    "torch.save(mean1_train, save_results_to+'mean1_train.pt')\n",
    "torch.save(std1_train, save_results_to+'std1_train.pt')\n",
    "torch.save(mean2_train, save_results_to+'mean2_train.pt')\n",
    "torch.save(std2_train, save_results_to+'std2_train.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
